{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TspH1wJmAkdk"
      },
      "source": [
        "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
        "\n",
        "The dataset can be used both for regression and classification tasks.\n",
        "\n",
        "#### Source:\n",
        "\n",
        "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
        "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Pedro Sernadela Universidade de Aveiro\n",
        "\n",
        "#### Data Set Information:\n",
        "\n",
        "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
        "* Acquisition date: January 8, 2015\n",
        "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
        "\n",
        "Attribute Information:\n",
        "0. url: URL of the article (non-predictive)\n",
        "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
        "2. n_tokens_title: Number of words in the title\n",
        "3. n_tokens_content: Number of words in the content\n",
        "4. n_unique_tokens: Rate of unique words in the content\n",
        "5. n_non_stop_words: Rate of non-stop words in the content\n",
        "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
        "7. num_hrefs: Number of links\n",
        "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
        "9. num_imgs: Number of images\n",
        "10. num_videos: Number of videos\n",
        "11. average_token_length: Average length of the words in the content\n",
        "12. num_keywords: Number of keywords in the metadata\n",
        "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
        "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
        "15. data_channel_is_bus: Is data channel 'Business'?\n",
        "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
        "17. data_channel_is_tech: Is data channel 'Tech'?\n",
        "18. data_channel_is_world: Is data channel 'World'?\n",
        "19. kw_min_min: Worst keyword (min. shares)\n",
        "20. kw_max_min: Worst keyword (max. shares)\n",
        "21. kw_avg_min: Worst keyword (avg. shares)\n",
        "22. kw_min_max: Best keyword (min. shares)\n",
        "23. kw_max_max: Best keyword (max. shares)\n",
        "24. kw_avg_max: Best keyword (avg. shares)\n",
        "25. kw_min_avg: Avg. keyword (min. shares)\n",
        "26. kw_max_avg: Avg. keyword (max. shares)\n",
        "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
        "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
        "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
        "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
        "31. weekday_is_monday: Was the article published on a Monday?\n",
        "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
        "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
        "34. weekday_is_thursday: Was the article published on a Thursday?\n",
        "35. weekday_is_friday: Was the article published on a Friday?\n",
        "36. weekday_is_saturday: Was the article published on a Saturday?\n",
        "37. weekday_is_sunday: Was the article published on a Sunday?\n",
        "38. is_weekend: Was the article published on the weekend?\n",
        "39. LDA_00: Closeness to LDA topic 0\n",
        "40. LDA_01: Closeness to LDA topic 1\n",
        "41. LDA_02: Closeness to LDA topic 2\n",
        "42. LDA_03: Closeness to LDA topic 3\n",
        "43. LDA_04: Closeness to LDA topic 4\n",
        "44. global_subjectivity: Text subjectivity\n",
        "45. global_sentiment_polarity: Text sentiment polarity\n",
        "46. global_rate_positive_words: Rate of positive words in the content\n",
        "47. global_rate_negative_words: Rate of negative words in the content\n",
        "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
        "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
        "50. avg_positive_polarity: Avg. polarity of positive words\n",
        "51. min_positive_polarity: Min. polarity of positive words\n",
        "52. max_positive_polarity: Max. polarity of positive words\n",
        "53. avg_negative_polarity: Avg. polarity of negative words\n",
        "54. min_negative_polarity: Min. polarity of negative words\n",
        "55. max_negative_polarity: Max. polarity of negative words\n",
        "56. title_subjectivity: Title subjectivity\n",
        "57. title_sentiment_polarity: Title polarity\n",
        "58. abs_title_subjectivity: Absolute subjectivity level\n",
        "59. abs_title_sentiment_polarity: Absolute polarity level\n",
        "60. shares: Number of shares (target)\n",
        "\n",
        "\n",
        "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
        "\n",
        "The last column **shares** contains the value to predict.\n",
        "\n",
        "### Regression\n",
        "In the case of regression we want to predict the value of the share column.\n",
        "\n",
        "### Classification\n",
        "In the case of classification we want to predict one of two classes:\n",
        "\n",
        "* *low* -- shares < 1,400\n",
        "* *high* -- shares >= 1,400\n",
        "\n",
        "### Metrics\n",
        "\n",
        "#### Regression\n",
        "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
        "\n",
        "$$\n",
        "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "* $n$ is the number of test samples\n",
        "* $d_i$ is the ground truth value of the i-th sample\n",
        "* $f_i$ is the predicted value of the i-th sample\n",
        "\n",
        "\n",
        "#### Classification\n",
        "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
        "\n",
        "$$\n",
        "\\frac{TP+TN}{TP+TN+FP+FN}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* TP is the number of *correctly* classified positive samples\n",
        "* TN is the number of *correctly* classified negative samples\n",
        "* FP is the number of *incorrectly* classified positive samples\n",
        "* FN is the number of *incorrectly* classified negative samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oesd6_bYijRo",
        "outputId": "e65246a5-0e1b-4bed-b0e2-d677d3df86d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-01 07:54:34--  https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7476401 (7.1M) [application/x-httpd-php]\n",
            "Saving to: ‘OnlineNewsPopularity.zip’\n",
            "\n",
            "OnlineNewsPopularit 100%[===================>]   7.13M  16.0MB/s    in 0.4s    \n",
            "\n",
            "2023-06-01 07:54:34 (16.0 MB/s) - ‘OnlineNewsPopularity.zip’ saved [7476401/7476401]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmad6QdZ_nFR",
        "outputId": "dea9debb-7eae-4a67-badd-80ea339b5429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  OnlineNewsPopularity.zip\n",
            "   creating: OnlineNewsPopularity/\n",
            "  inflating: OnlineNewsPopularity/OnlineNewsPopularity.names  \n",
            "  inflating: OnlineNewsPopularity/OnlineNewsPopularity.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eW4t_c6ACcm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXCX_LpFedtj"
      },
      "source": [
        "Format properly the names of the columns and remove the first two columns\n",
        "\n",
        "1.   List item\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mxntjhmAH0D"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[: , 2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCXwyljnAMmi",
        "outputId": "47e6de3f-1b4e-4a73-898e-6e587a4c7b4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "n_tokens_title                   float64\n",
              "n_tokens_content                 float64\n",
              "n_unique_tokens                  float64\n",
              "n_non_stop_words                 float64\n",
              "n_non_stop_unique_tokens         float64\n",
              "num_hrefs                        float64\n",
              "num_self_hrefs                   float64\n",
              "num_imgs                         float64\n",
              "num_videos                       float64\n",
              "average_token_length             float64\n",
              "num_keywords                     float64\n",
              "data_channel_is_lifestyle        float64\n",
              "data_channel_is_entertainment    float64\n",
              "data_channel_is_bus              float64\n",
              "data_channel_is_socmed           float64\n",
              "data_channel_is_tech             float64\n",
              "data_channel_is_world            float64\n",
              "kw_min_min                       float64\n",
              "kw_max_min                       float64\n",
              "kw_avg_min                       float64\n",
              "kw_min_max                       float64\n",
              "kw_max_max                       float64\n",
              "kw_avg_max                       float64\n",
              "kw_min_avg                       float64\n",
              "kw_max_avg                       float64\n",
              "kw_avg_avg                       float64\n",
              "self_reference_min_shares        float64\n",
              "self_reference_max_shares        float64\n",
              "self_reference_avg_sharess       float64\n",
              "weekday_is_monday                float64\n",
              "weekday_is_tuesday               float64\n",
              "weekday_is_wednesday             float64\n",
              "weekday_is_thursday              float64\n",
              "weekday_is_friday                float64\n",
              "weekday_is_saturday              float64\n",
              "weekday_is_sunday                float64\n",
              "is_weekend                       float64\n",
              "LDA_00                           float64\n",
              "LDA_01                           float64\n",
              "LDA_02                           float64\n",
              "LDA_03                           float64\n",
              "LDA_04                           float64\n",
              "global_subjectivity              float64\n",
              "global_sentiment_polarity        float64\n",
              "global_rate_positive_words       float64\n",
              "global_rate_negative_words       float64\n",
              "rate_positive_words              float64\n",
              "rate_negative_words              float64\n",
              "avg_positive_polarity            float64\n",
              "min_positive_polarity            float64\n",
              "max_positive_polarity            float64\n",
              "avg_negative_polarity            float64\n",
              "min_negative_polarity            float64\n",
              "max_negative_polarity            float64\n",
              "title_subjectivity               float64\n",
              "title_sentiment_polarity         float64\n",
              "abs_title_subjectivity           float64\n",
              "abs_title_sentiment_polarity     float64\n",
              "shares                             int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu_rxzq0f2gV"
      },
      "source": [
        "## Let's plot some of the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VbFlnYRjAXlk",
        "outputId": "bf6a852a-9752-48ff-a22e-ee983d6a7f17"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x2000 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "columns_to_plot = [\n",
        "    'num_videos',\n",
        "    'num_imgs',\n",
        "    'num_keywords',\n",
        "    'data_channel_is_world',\n",
        "    'rate_negative_words',\n",
        "    'self_reference_avg_sharess',\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots(len(columns_to_plot), 1, figsize=(20, 20))\n",
        "\n",
        "for i, column in enumerate(columns_to_plot, 0):\n",
        "  ax[i].hist(df[column])\n",
        "  ax[i].title.set_text(column)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Ko8Lvxa3f8"
      },
      "source": [
        "#Considerazioni sui dati\n",
        "\n",
        "Il numero totale di campioni a disposizione è di 39644. Questo numero che può sembrare grande non lo è in realtà così tanto nel contesto in cui ci troviamo, considerando che andrà poi diviso ulteriormente in un Training Set, un Validation Set ed un Test Set. E' possibile quindi che alcuni algoritmi mostrino performance migliori usando una strategia di **Cross Validation** in fase di hyperparameter tuning.\n",
        "\n",
        "I campioni inoltre hanno 61 feature in totale, di cui 1, la colonna 'shares' è il nostro target, altre 58 sono utili al fine di predirne il valore, di cui molte correlate tra loro, e 2 sono non predittive. Un numero così alto di features mostra una complessità elevata del dataset, che potrebbe quindi richiedere un modello predittivo altrettanto complesso. Il rischio quindi è quello che, con una regolarizzazione troppo aggressiva, si verifichi **underfitting**.\n",
        "\n",
        "> **Nota**: il nostro dataset è composto da feature con valori continui. Per essere sottoposto a problemi di classificazione verrà discretizzato parzialmente o totalmente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgLxm99yukH-"
      },
      "source": [
        "#Decision Tree\n",
        "\n",
        "I Decision Tree sono un tipo di algoritmo di machine learning utile nei problemi di classificazione. Ad ogni nodo dell'albero, dato un campione di cui si vuole predirre il target, si sceglie in quale dei figli del nodo attuale spostarsi a seconda del valore di una feature. Visto che i nodi dell'albero devono essere finiti, anche i valori possibili delle feature dei campioni devono essere finiti.\n",
        "\n",
        "In particolare il seguente decision tree è binario (ogni nodo ha due figli al massimo), per cui ogni feature è stata discretizzata sui valori 0 o 1, a seconda se il valore originare fosse inferiore o superiore alla mediana dei valori di quella feature su tutti i campioni."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "MzVnLaojQ4_c",
        "outputId": "9f7fbfdd-a65c-412d-b185-d79703eb92f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-40e04a74-a1dd-49f4-b3a9-ccb4625cb882\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>num_hrefs</th>\n",
              "      <th>num_self_hrefs</th>\n",
              "      <th>num_imgs</th>\n",
              "      <th>num_videos</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>...</th>\n",
              "      <th>min_positive_polarity</th>\n",
              "      <th>max_positive_polarity</th>\n",
              "      <th>avg_negative_polarity</th>\n",
              "      <th>min_negative_polarity</th>\n",
              "      <th>max_negative_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "      <th>abs_title_subjectivity</th>\n",
              "      <th>abs_title_sentiment_polarity</th>\n",
              "      <th>shares</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39639</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39640</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39641</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39642</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39643</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39644 rows × 59 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-40e04a74-a1dd-49f4-b3a9-ccb4625cb882')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-40e04a74-a1dd-49f4-b3a9-ccb4625cb882 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-40e04a74-a1dd-49f4-b3a9-ccb4625cb882');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
              "0                   1                 0                1                 0   \n",
              "1                   0                 0                1                 0   \n",
              "2                   0                 0                1                 0   \n",
              "3                   0                 1                0                 1   \n",
              "4                   1                 1                0                 1   \n",
              "...               ...               ...              ...               ...   \n",
              "39639               1                 0                0                 0   \n",
              "39640               1                 0                1                 0   \n",
              "39641               1                 1                0                 1   \n",
              "39642               0                 1                1                 1   \n",
              "39643               1                 0                1                 0   \n",
              "\n",
              "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
              "0                             1          0               0         1   \n",
              "1                             1          0               0         1   \n",
              "2                             0          0               0         1   \n",
              "3                             0          1               0         1   \n",
              "4                             0          1               1         1   \n",
              "...                         ...        ...             ...       ...   \n",
              "39639                         0          1               1         1   \n",
              "39640                         1          1               1         1   \n",
              "39641                         0          1               0         1   \n",
              "39642                         1          1               0         1   \n",
              "39643                         1          0               0         0   \n",
              "\n",
              "       num_videos  average_token_length  ...  min_positive_polarity  \\\n",
              "0               1                     1  ...                      1   \n",
              "1               1                     1  ...                      0   \n",
              "2               1                     0  ...                      1   \n",
              "3               1                     0  ...                      1   \n",
              "4               1                     1  ...                      0   \n",
              "...           ...                   ...  ...                    ...   \n",
              "39639           1                     0  ...                      1   \n",
              "39640           1                     0  ...                      1   \n",
              "39641           1                     1  ...                      1   \n",
              "39642           1                     1  ...                      0   \n",
              "39643           1                     0  ...                      1   \n",
              "\n",
              "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
              "0                          0                      0                      0   \n",
              "1                          0                      1                      1   \n",
              "2                          1                      0                      0   \n",
              "3                          1                      0                      0   \n",
              "4                          1                      1                      1   \n",
              "...                      ...                    ...                    ...   \n",
              "39639                      0                      0                      1   \n",
              "39640                      0                      1                      1   \n",
              "39641                      0                      0                      0   \n",
              "39642                      0                      1                      1   \n",
              "39643                      0                      1                      1   \n",
              "\n",
              "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
              "0                          0                   1                         0   \n",
              "1                          1                   0                         1   \n",
              "2                          0                   0                         1   \n",
              "3                          0                   0                         1   \n",
              "4                          1                   1                         1   \n",
              "...                      ...                 ...                       ...   \n",
              "39639                      0                   0                         1   \n",
              "39640                      1                   1                         1   \n",
              "39641                      0                   1                         1   \n",
              "39642                      1                   0                         1   \n",
              "39643                      0                   1                         1   \n",
              "\n",
              "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
              "0                           0                             1       0  \n",
              "1                           1                             1       0  \n",
              "2                           1                             1       1  \n",
              "3                           1                             1       0  \n",
              "4                           0                             1       0  \n",
              "...                       ...                           ...     ...  \n",
              "39639                       0                             1       1  \n",
              "39640                       0                             1       1  \n",
              "39641                       0                             1       1  \n",
              "39642                       1                             1       0  \n",
              "39643                       0                             1       0  \n",
              "\n",
              "[39644 rows x 59 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "df_binary = pd.DataFrame()\n",
        "\n",
        "for column in df.columns:\n",
        "    median = df[column].median()\n",
        "    df_binary[column] = np.where(df[column] >= median, 1, 0)\n",
        "\n",
        "display(df_binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_wTCXnemfqU"
      },
      "source": [
        "**Splitting**\n",
        "\n",
        "Successivamente il dataset viene diviso in Training, Validation e Test set. Le percentuali sono rispettivamente l'80%, il 10% e il 10% del dataset originale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbn9JLiDmYkR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DT_train, DT_test = train_test_split(df_binary, test_size=0.8, random_state = 25) #il randomstate è il seed per uno shuffle randomico del dataset prima di essere diviso.\n",
        "                                                                                  #in questo modo si evita che campioni vicini e potenzialmente correlati finiscano nello stesso insieme\n",
        "DT_test, DT_validation = train_test_split(DT_test, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7X_r0q2pU-x"
      },
      "source": [
        "**Principi**\n",
        "\n",
        "L'Implementazione del Decision Tree si serve di una funzione che calcola l'Entropia. Ogni nodo interno rappresenta una feature, i nodi foglie invece contengono un valore che rappresenta la risposta (0 o 1) a cui il percorso sull'albero ha condotto. Ogni nodo segna un decision boundary nello spazio delle features.\n",
        "\n",
        "L'obiettivo è suddividere lo spazio in party all'interno delle quali l'entropia calcolata sulla feature 'shares' sia minima (ovvero i suoi sono quasi tutti simili).\n",
        "\n",
        "In questo modo si può stimare con più o meno precisione che se un punto cade in un sottospazio, allora avrà quella label\n",
        "\n",
        "La funzione find_argmax_Gain serve per scegliere, durante la costruzione dell'albero, di volta in volta la feature che introduce il decision boundary migliore in termini di entropia che resta ad entrambi i suoi lati."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTZUfWDDumWu"
      },
      "outputs": [],
      "source": [
        "class TreeNode:\n",
        "  def __init__(self, feature, left, right, value):\n",
        "    self.feature = feature    #è null nelle foglie\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value        #è null nei nodi interni\n",
        "\n",
        "def Entropy(data):            #calcola l'entropia effettuando dei controlli preventivi su sitazioni che portano l'entropia ad essere automaticamente 0 e poi applica la formula se necessario\n",
        "  if data.empty:\n",
        "    return 0                  #se l'insieme è vuoto, entropia=0\n",
        "\n",
        "  count = data.values.sum()\n",
        "  if count == 0 : return 0    #se tutte le label sono uguali a 0, entropia=0\n",
        "\n",
        "  n = len(data)\n",
        "  if n == count: return 0     #se tutte le label sono uguali a 1 e quindi sommano a len, entropia=0\n",
        "  p = count/n\n",
        "  return -(p*np.log2(p) + (1-p)*np.log2(1-p))\n",
        "\n",
        "def Remainder(data, attribute, target):                           #calcola l'entropia sulla colonna target che rimane dopo aver introdotto il decision boundary corrispondete alla feature 'attribute'\n",
        "  ret = 0\n",
        "  for value in [0,1]:\n",
        "    subset = data[ data[attribute] == value ]                     #divide l'insieme in un sottoinsieme con solo valori 0 e uno con solo valori 1 nella colonna attribute\n",
        "    ret += len(subset)/len(data) * Entropy(subset.get(target))    #somma le entropie dei sottoinsiemi pesandole con la frazione che ogni sottoinsieme rappresenta dell'insieme precedente\n",
        "  return ret\n",
        "\n",
        "def Gain(data, attribute, target):                                    #calcola il guadagno in termini di entropia dato dalla feature 'attribute'\n",
        "  return Entropy(data[target]) - Remainder(data, attribute, target)\n",
        "\n",
        "def find_argmax_Gain(data, target):\n",
        "  gains = [Gain(data, attribute, target) for attribute in data.columns if attribute != 'shares']  #calcola il gain dato da ciascuna feature disponibile\n",
        "  max_index = np.argmax(gains)\n",
        "  argmax = data.columns[max_index]                                                                #sceglie il migliore\n",
        "  return argmax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80AcUgKQtqW8"
      },
      "source": [
        "**Training**\n",
        "\n",
        "La funzione build_Tree rappresenta la fase di training dell'albero ovvero la costruzione vera e propria, utilizzando le feature che meglio classificano il dataset in oggetto.\n",
        "\n",
        "La funzione printTree è utile per visualizzarne il risultato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMbP8Vn7RCe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_tree(data, target, maxdepth):\n",
        "\n",
        "  if data.empty:\n",
        "    return None\n",
        "  e = Entropy(data.get(target))\n",
        "  if(  e == 0 ):\n",
        "    return TreeNode(None, None, None, data[target].iloc[0])\n",
        "  if( len(data.columns) == 1 or maxdepth==1 ):\n",
        "    return TreeNode(None, None, None, data[target].mode()[0])\n",
        "\n",
        "  feature = find_argmax_Gain(data, target);\n",
        "\n",
        "  left = build_tree( data[ data[feature] == 0 ].drop(feature, inplace=False, axis=1), target, maxdepth-1)\n",
        "  right = build_tree( data[ data[feature] == 1 ].drop(feature, inplace=False, axis=1), target, maxdepth-1)\n",
        "\n",
        "  return TreeNode(feature, left, right, None)\n",
        "\n",
        "def printTree(node, level=0):\n",
        "    if node != None:\n",
        "      printTree(node.left, level + 1)\n",
        "      print(' ' * 4 * level + '-> ' + str(node.feature))\n",
        "      printTree(node.right, level + 1)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElVeRrv7uz4u"
      },
      "source": [
        "**Prediction**\n",
        "\n",
        "La funzione di predizione consiste in una visita in profondità fino ad arrivare ad una foglia dell'albero. Non è possibile giungere in più di una foglia.\n",
        "\n",
        "> Ogni scelta effetuata in un nodo esclude tutte le altre possibilita\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq3f42g4uj1C"
      },
      "outputs": [],
      "source": [
        "def predict(node, instance):\n",
        "\n",
        "  if node.value != None:\n",
        "    return node.value\n",
        "\n",
        "  if instance[node.feature] == 0:\n",
        "    return predict(node.left, instance)\n",
        "  else:\n",
        "    return predict(node.right, instance)\n",
        "\n",
        "DT_validation = DT_validation.reset_index(drop=True)\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnrwE2Cy5z07",
        "outputId": "ab87a5fd-46b5-48f0-88a9-4784bf0ac4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provando profondità: \n",
            "2\n",
            "Accuracy media sui fold: 0.5952911498843809\n",
            "Provando profondità: \n",
            "3\n",
            "Accuracy media sui fold: 0.6015135589657347\n",
            "Provando profondità: \n",
            "5\n",
            "Accuracy media sui fold: 0.6174900147151566\n",
            "Provando profondità: \n",
            "7\n",
            "Accuracy media sui fold: 0.6196762665545512\n",
            "Provando profondità: \n",
            "9\n",
            "Accuracy media sui fold: 0.6145049400882909\n",
            "Provando profondità: \n",
            "10\n",
            "Accuracy media sui fold: 0.604078200546563\n",
            "============================================\n",
            "Migliore accuracy sul validation set: 0.6196762665545512 con profondità 7\n"
          ]
        }
      ],
      "source": [
        "# Define the number of folds for cross-validation\n",
        "n_folds = 5\n",
        "tutto = pd.concat([DT_train, DT_validation], axis=0, ignore_index=True)\n",
        "# Create an array of indices to shuffle and split the data into folds\n",
        "indices = np.arange(len(tutto))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Split the data into folds\n",
        "fold_size = len(tutto) // n_folds\n",
        "folds = [indices[i:i+fold_size] for i in range(0, len(tutto), fold_size)]\n",
        "\n",
        "# Perform cross-validation\n",
        "\n",
        "\n",
        "depths = [2,3,5,7,9,10]\n",
        "\n",
        "best_acc = 0\n",
        "best_depth = 2\n",
        "\n",
        "for depth in depths:\n",
        "  scores = []\n",
        "  print(\"Provando profondità: \")\n",
        "  print(depth)\n",
        "  for i in range(n_folds):\n",
        "      # Get the training and testing indices for the current fold\n",
        "      validation_indices = folds[i]\n",
        "      train_indices = np.concatenate(folds[:i] + folds[i+1:])\n",
        "\n",
        "      # Split the data into training and testing sets\n",
        "      DT_train = tutto.iloc[train_indices]\n",
        "      DT_validation = tutto.iloc[validation_indices]\n",
        "      tree = build_tree(DT_train, 'shares', depth)\n",
        "      predictions = pd.DataFrame(columns = ['shares'])\n",
        "      predictions = DT_validation.apply(lambda row: predict(tree, row), axis=1)\n",
        "      current_acc = metrics.accuracy_score(DT_validation['shares'], predictions)\n",
        "\n",
        "      scores.append(current_acc)\n",
        "\n",
        "  # Calculate the average score across all folds\n",
        "  mean_score = np.mean(scores)\n",
        "\n",
        "  # Print the mean score\n",
        "  print(\"Accuracy media sui fold:\", mean_score)\n",
        "\n",
        "  if mean_score > best_acc :\n",
        "    best_acc = mean_score\n",
        "    best_depth = depth\n",
        "\n",
        "print(\"============================================\")\n",
        "print(\"Migliore accuracy sul validation set:\", best_acc, 'con profondità', best_depth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxEaRTMyp2b"
      },
      "source": [
        "**Considerazioni sui risultati della Cross Validation**\n",
        "\n",
        "E' stata usata una 5-fold Cross Validation per scegliere il miglior valore di profondità dell'albero. Nonostante infatti un albero con tanti livelli può ricalcare più finamente i dati di training, con altrettanta facilità può andare in overfitting.\n",
        "\n",
        "Come previsto per un modello del tipo dei Decision Tree, l'andamento dell' errore sul Validation Set, segue un **andamento ad U** (manifestato da un andamento inverso \"ad arco\" dell'accuracy), ottentendo il risultato migliore con una profondità di 7 livelli. Dopo quel valore l'accuracy sul validation Set inizia a diminuire perchè l'albero non generalizza più così bene su dati che non erano nel Training Set, mentre diventa sempre più preciso sul quest'ultimo.\n",
        "\n",
        "D'altronde non ci aspettavamo un andamento Double dip perchè a differenza di altri modelli, aumentare la complessità di un decision tree, non apre le porte a potenziali ulteriori rappresentazioni del problema, come succede invece aumentando il numero di neuroni di una rete neurale per esempio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq-9DM28U_hD",
        "outputId": "6e9b6a15-169a-4184-9317-c13baf195c10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        -> None\n",
            "                    -> num_imgs\n",
            "                        -> None\n",
            "                -> kw_avg_min\n",
            "                        -> None\n",
            "                    -> kw_avg_max\n",
            "                        -> None\n",
            "            -> LDA_04\n",
            "                        -> None\n",
            "                    -> kw_avg_min\n",
            "                        -> None\n",
            "                -> num_self_hrefs\n",
            "                        -> None\n",
            "                    -> max_positive_polarity\n",
            "                        -> None\n",
            "        -> kw_max_max\n",
            "                        -> None\n",
            "                    -> self_reference_min_shares\n",
            "                        -> None\n",
            "                -> kw_max_avg\n",
            "                        -> None\n",
            "                    -> LDA_00\n",
            "                        -> None\n",
            "            -> LDA_04\n",
            "                        -> None\n",
            "                    -> max_negative_polarity\n",
            "                        -> None\n",
            "                -> average_token_length\n",
            "                        -> None\n",
            "                    -> kw_max_avg\n",
            "                        -> None\n",
            "    -> self_reference_avg_sharess\n",
            "                        -> None\n",
            "                    -> max_positive_polarity\n",
            "                        -> None\n",
            "                -> kw_max_max\n",
            "                        -> None\n",
            "                    -> global_sentiment_polarity\n",
            "                        -> None\n",
            "            -> LDA_04\n",
            "                        -> None\n",
            "                    -> n_tokens_title\n",
            "                        -> None\n",
            "                -> n_unique_tokens\n",
            "                        -> None\n",
            "                    -> kw_min_max\n",
            "                        -> None\n",
            "        -> self_reference_min_shares\n",
            "                        -> None\n",
            "                    -> rate_positive_words\n",
            "                        -> None\n",
            "                -> kw_max_max\n",
            "                        -> None\n",
            "                    -> num_hrefs\n",
            "                        -> None\n",
            "            -> kw_min_max\n",
            "                        -> None\n",
            "                    -> num_imgs\n",
            "                        -> None\n",
            "                -> title_sentiment_polarity\n",
            "                        -> None\n",
            "                    -> LDA_04\n",
            "                        -> None\n",
            "-> kw_avg_avg\n",
            "                        -> None\n",
            "                    -> LDA_03\n",
            "                        -> None\n",
            "                -> max_positive_polarity\n",
            "                        -> None\n",
            "                    -> n_non_stop_words\n",
            "                        -> None\n",
            "            -> kw_max_max\n",
            "                        -> None\n",
            "                    -> kw_max_avg\n",
            "                        -> None\n",
            "                -> min_positive_polarity\n",
            "                        -> None\n",
            "                    -> self_reference_max_shares\n",
            "                        -> None\n",
            "        -> num_hrefs\n",
            "                        -> None\n",
            "                    -> LDA_01\n",
            "                        -> None\n",
            "                -> n_unique_tokens\n",
            "                        -> None\n",
            "                    -> n_tokens_title\n",
            "                        -> None\n",
            "            -> LDA_02\n",
            "                        -> None\n",
            "                    -> rate_positive_words\n",
            "                        -> None\n",
            "                -> self_reference_max_shares\n",
            "                        -> None\n",
            "                    -> num_keywords\n",
            "                        -> None\n",
            "    -> self_reference_min_shares\n",
            "                        -> None\n",
            "                    -> LDA_04\n",
            "                        -> None\n",
            "                -> LDA_02\n",
            "                        -> None\n",
            "                    -> min_positive_polarity\n",
            "                        -> None\n",
            "            -> LDA_01\n",
            "                        -> None\n",
            "                    -> n_tokens_title\n",
            "                        -> None\n",
            "                -> self_reference_max_shares\n",
            "                        -> None\n",
            "                    -> min_positive_polarity\n",
            "                        -> None\n",
            "        -> num_hrefs\n",
            "                        -> None\n",
            "                    -> n_non_stop_words\n",
            "                        -> None\n",
            "                -> global_subjectivity\n",
            "                        -> None\n",
            "                    -> num_self_hrefs\n",
            "                        -> None\n",
            "            -> self_reference_avg_sharess\n",
            "                        -> None\n",
            "                    -> global_rate_positive_words\n",
            "                        -> None\n",
            "                -> LDA_03\n",
            "                        -> None\n",
            "                    -> num_keywords\n",
            "                        -> None\n",
            "Accuracy sul test set con profondità 7\n",
            "0.6143271534871989\n"
          ]
        }
      ],
      "source": [
        "tree = build_tree(pd.concat([DT_train, DT_validation], axis=0), 'shares', best_depth)\n",
        "printTree(tree)\n",
        "predictions = pd.DataFrame(columns = ['shares'])\n",
        "\n",
        "predictions = DT_test.apply(lambda row: predict(tree, row), axis=1)\n",
        "\n",
        "test_acc_dt = metrics.accuracy_score(DT_test['shares'], predictions)\n",
        "print(\"Accuracy sul test set con profondità\", best_depth)\n",
        "print(test_acc_dt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUc9_i5G1QEl"
      },
      "source": [
        "**Considerazioni sul risultato delle predizioni**\n",
        "\n",
        "A fronte di una accuracy sul Validation Set (media) del 61,76%, abbiamo ottenuto una accuracy sul Test Set del 61,43%.\n",
        "\n",
        "Possiamo quindi dire che il modello implementato generalizza abbastanza bene su dati che non ha visto in fase di training, dal momento che la caduta di accuracy è minima e potrebbe rientrare in una varianza fisiologica.\n",
        "\n",
        "> Il modello non sembra overfittare.\n",
        "\n",
        "Controlliamo se a parità di iperparametri, l'implementazione di scikitlean performa meglio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AenyxDnd_HX",
        "outputId": "c40eb407-0fd3-4d58-9585-4558744c2136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn Accuracy con profondità 3 : 0.5996973136587211\n",
            "Sklearn Accuracy con profondità 5 : 0.611741707655442\n",
            "Sklearn Accuracy con profondità 7 : 0.6142640938327658\n",
            "Sklearn Accuracy con profondità 9 : 0.6074536511539916\n",
            "Sklearn Accuracy con profondità 15 : 0.5568167486442175\n",
            "Sklearn Accuracy con profondità 20 : 0.5467272039349225\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "sklearn_acc_dt=0\n",
        "for d in [3, 5, 7, 9, 15, 20]:\n",
        "  classifier = DecisionTreeClassifier(max_depth=d)\n",
        "  classifier.fit(pd.concat([DT_train, DT_validation], axis=0)[feature_names], pd.concat([DT_train, DT_validation], axis=0)[target_name])\n",
        "  sklearn_predictions = classifier.predict(DT_test[feature_names])\n",
        "  sklearn_accuracy = metrics.accuracy_score(DT_test[target_name], sklearn_predictions)\n",
        "  print(\"Sklearn Accuracy con profondità\", d, \":\", sklearn_accuracy)\n",
        "  if(d==7):\n",
        "    sklearn_acc_dt = sklearn_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFwQuzvD23IG"
      },
      "source": [
        "L'implementazione di sklearn presenta un accuracy praticamente uguale alla nostra usando la stessa maxdepth. Provando con profondità diverse si può osservare che anche per l'implementazione di sklearn l'iperparametro migliore è 7.\n",
        "Questo è sintomo di una corretta implementazione sia del Decision Tree, sia del processo di Validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZeeiH7a6Oii"
      },
      "source": [
        "# Linear Regression senza regolarizzazione\n",
        "\n",
        "##Considerazioni sul preprocessing dei dati\n",
        "\n",
        "La regressione lineare non necessita la discretizzazione nè della feature target, nè delle feature predittive.\n",
        "\n",
        "Quello che però viene fatto, dopo aver diviso il dataset in predittori e target perchè usare matrici diverse è più agevole per le formule che verranno applicate, è normalizzare le feature.\n",
        "\n",
        "La normalizzazione serve per portare tutte le feature ad avere una media di 0 e una variazione standard di 1 attraverso tutti i campioni. In questo modo tutte le feature avranno valori tra loro comparabili e si supererà l'ostacolo di dover lavorare con feature che hanno valori molto piccoli come title_sentiment_polarity e altre che ospitano valori grandi come n_tokens_content.\n",
        "\n",
        "> **Nota:**La colonna target non è stata normalizzata perchè vogliamo stimarla nel suo valore reale. Se la normalizzassimo, il gradient descent troverebbe il vettore dei pesi che stima il valore normalizzato.\n",
        "\n",
        "La seguente cella di codice mostra il dataset prima della normalizzazione. La cella di codice ancora successiva, mostra invece i dati normalizzati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "nm5EKhDY6ZDH",
        "outputId": "25e2d04e-8ad9-4d79-9806-35a735f10f0d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b7966137-e2a0-43e2-8424-066d90a66450\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>num_hrefs</th>\n",
              "      <th>num_self_hrefs</th>\n",
              "      <th>num_imgs</th>\n",
              "      <th>num_videos</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>...</th>\n",
              "      <th>min_positive_polarity</th>\n",
              "      <th>max_positive_polarity</th>\n",
              "      <th>avg_negative_polarity</th>\n",
              "      <th>min_negative_polarity</th>\n",
              "      <th>max_negative_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "      <th>abs_title_subjectivity</th>\n",
              "      <th>abs_title_sentiment_polarity</th>\n",
              "      <th>shares</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.663594</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.815385</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.680365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.350000</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>0.604743</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.791946</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.913725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.118750</td>\n",
              "      <td>-0.125</td>\n",
              "      <td>-0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>0.575130</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.663866</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.393365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1.00</td>\n",
              "      <td>-0.466667</td>\n",
              "      <td>-0.800</td>\n",
              "      <td>-0.133333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9.0</td>\n",
              "      <td>531.0</td>\n",
              "      <td>0.503788</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.665635</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.404896</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.80</td>\n",
              "      <td>-0.369697</td>\n",
              "      <td>-0.600</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.0</td>\n",
              "      <td>1072.0</td>\n",
              "      <td>0.415646</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.540890</td>\n",
              "      <td>19.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.682836</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>1.00</td>\n",
              "      <td>-0.220192</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.050000</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39639</th>\n",
              "      <td>11.0</td>\n",
              "      <td>346.0</td>\n",
              "      <td>0.529052</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.684783</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.523121</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.75</td>\n",
              "      <td>-0.260000</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.125000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39640</th>\n",
              "      <td>12.0</td>\n",
              "      <td>328.0</td>\n",
              "      <td>0.696296</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.885057</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>4.405488</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.70</td>\n",
              "      <td>-0.211111</td>\n",
              "      <td>-0.400</td>\n",
              "      <td>-0.100000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39641</th>\n",
              "      <td>10.0</td>\n",
              "      <td>442.0</td>\n",
              "      <td>0.516355</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.644128</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.076923</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.356439</td>\n",
              "      <td>-0.800</td>\n",
              "      <td>-0.166667</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>0.136364</td>\n",
              "      <td>1900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39642</th>\n",
              "      <td>6.0</td>\n",
              "      <td>682.0</td>\n",
              "      <td>0.539493</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.692661</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.975073</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.205246</td>\n",
              "      <td>-0.500</td>\n",
              "      <td>-0.012500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39643</th>\n",
              "      <td>10.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>0.701987</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.471338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.50</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>-0.200</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39644 rows × 59 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7966137-e2a0-43e2-8424-066d90a66450')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7966137-e2a0-43e2-8424-066d90a66450 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7966137-e2a0-43e2-8424-066d90a66450');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
              "0                12.0             219.0         0.663594               1.0   \n",
              "1                 9.0             255.0         0.604743               1.0   \n",
              "2                 9.0             211.0         0.575130               1.0   \n",
              "3                 9.0             531.0         0.503788               1.0   \n",
              "4                13.0            1072.0         0.415646               1.0   \n",
              "...               ...               ...              ...               ...   \n",
              "39639            11.0             346.0         0.529052               1.0   \n",
              "39640            12.0             328.0         0.696296               1.0   \n",
              "39641            10.0             442.0         0.516355               1.0   \n",
              "39642             6.0             682.0         0.539493               1.0   \n",
              "39643            10.0             157.0         0.701987               1.0   \n",
              "\n",
              "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
              "0                      0.815385        4.0             2.0       1.0   \n",
              "1                      0.791946        3.0             1.0       1.0   \n",
              "2                      0.663866        3.0             1.0       1.0   \n",
              "3                      0.665635        9.0             0.0       1.0   \n",
              "4                      0.540890       19.0            19.0      20.0   \n",
              "...                         ...        ...             ...       ...   \n",
              "39639                  0.684783        9.0             7.0       1.0   \n",
              "39640                  0.885057        9.0             7.0       3.0   \n",
              "39641                  0.644128       24.0             1.0      12.0   \n",
              "39642                  0.692661       10.0             1.0       1.0   \n",
              "39643                  0.846154        1.0             1.0       0.0   \n",
              "\n",
              "       num_videos  average_token_length  ...  min_positive_polarity  \\\n",
              "0             0.0              4.680365  ...               0.100000   \n",
              "1             0.0              4.913725  ...               0.033333   \n",
              "2             0.0              4.393365  ...               0.100000   \n",
              "3             0.0              4.404896  ...               0.136364   \n",
              "4             0.0              4.682836  ...               0.033333   \n",
              "...           ...                   ...  ...                    ...   \n",
              "39639         1.0              4.523121  ...               0.100000   \n",
              "39640        48.0              4.405488  ...               0.136364   \n",
              "39641         1.0              5.076923  ...               0.136364   \n",
              "39642         0.0              4.975073  ...               0.062500   \n",
              "39643         2.0              4.471338  ...               0.100000   \n",
              "\n",
              "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
              "0                       0.70              -0.350000                 -0.600   \n",
              "1                       0.70              -0.118750                 -0.125   \n",
              "2                       1.00              -0.466667                 -0.800   \n",
              "3                       0.80              -0.369697                 -0.600   \n",
              "4                       1.00              -0.220192                 -0.500   \n",
              "...                      ...                    ...                    ...   \n",
              "39639                   0.75              -0.260000                 -0.500   \n",
              "39640                   0.70              -0.211111                 -0.400   \n",
              "39641                   0.50              -0.356439                 -0.800   \n",
              "39642                   0.50              -0.205246                 -0.500   \n",
              "39643                   0.50              -0.200000                 -0.200   \n",
              "\n",
              "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
              "0                  -0.200000            0.500000                 -0.187500   \n",
              "1                  -0.100000            0.000000                  0.000000   \n",
              "2                  -0.133333            0.000000                  0.000000   \n",
              "3                  -0.166667            0.000000                  0.000000   \n",
              "4                  -0.050000            0.454545                  0.136364   \n",
              "...                      ...                 ...                       ...   \n",
              "39639              -0.125000            0.100000                  0.000000   \n",
              "39640              -0.100000            0.300000                  1.000000   \n",
              "39641              -0.166667            0.454545                  0.136364   \n",
              "39642              -0.012500            0.000000                  0.000000   \n",
              "39643              -0.200000            0.333333                  0.250000   \n",
              "\n",
              "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
              "0                    0.000000                      0.187500     593  \n",
              "1                    0.500000                      0.000000     711  \n",
              "2                    0.500000                      0.000000    1500  \n",
              "3                    0.500000                      0.000000    1200  \n",
              "4                    0.045455                      0.136364     505  \n",
              "...                       ...                           ...     ...  \n",
              "39639                0.400000                      0.000000    1800  \n",
              "39640                0.200000                      1.000000    1900  \n",
              "39641                0.045455                      0.136364    1900  \n",
              "39642                0.500000                      0.000000    1100  \n",
              "39643                0.166667                      0.250000    1300  \n",
              "\n",
              "[39644 rows x 59 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "xc2guyiv6hJ-",
        "outputId": "611c3e28-3246-4291-c235-2ad1542952ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3e9d6844-1e40-449e-9e35-c87d1e30247a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>num_hrefs</th>\n",
              "      <th>num_self_hrefs</th>\n",
              "      <th>num_imgs</th>\n",
              "      <th>num_videos</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>...</th>\n",
              "      <th>avg_positive_polarity</th>\n",
              "      <th>min_positive_polarity</th>\n",
              "      <th>max_positive_polarity</th>\n",
              "      <th>avg_negative_polarity</th>\n",
              "      <th>min_negative_polarity</th>\n",
              "      <th>max_negative_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "      <th>abs_title_subjectivity</th>\n",
              "      <th>abs_title_sentiment_polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.757438</td>\n",
              "      <td>-0.695202</td>\n",
              "      <td>0.032771</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.038657</td>\n",
              "      <td>-0.607455</td>\n",
              "      <td>-0.335562</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.156472</td>\n",
              "      <td>...</td>\n",
              "      <td>0.237334</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-0.228938</td>\n",
              "      <td>-0.708361</td>\n",
              "      <td>-0.268891</td>\n",
              "      <td>-0.969874</td>\n",
              "      <td>0.671237</td>\n",
              "      <td>-0.975420</td>\n",
              "      <td>-1.810696</td>\n",
              "      <td>0.138918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.661648</td>\n",
              "      <td>-0.618786</td>\n",
              "      <td>0.016056</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.031478</td>\n",
              "      <td>-0.695700</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.432833</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.640032</td>\n",
              "      <td>-0.870957</td>\n",
              "      <td>-0.228938</td>\n",
              "      <td>1.102160</td>\n",
              "      <td>1.367406</td>\n",
              "      <td>0.078641</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.661648</td>\n",
              "      <td>-0.712183</td>\n",
              "      <td>0.007644</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.007752</td>\n",
              "      <td>-0.695700</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>-0.183412</td>\n",
              "      <td>...</td>\n",
              "      <td>1.358384</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>-1.621776</td>\n",
              "      <td>-0.957859</td>\n",
              "      <td>-0.270864</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.661648</td>\n",
              "      <td>-0.032932</td>\n",
              "      <td>-0.012619</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.007210</td>\n",
              "      <td>-0.166227</td>\n",
              "      <td>-0.854350</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>-0.169756</td>\n",
              "      <td>...</td>\n",
              "      <td>0.307438</td>\n",
              "      <td>0.573766</td>\n",
              "      <td>0.174637</td>\n",
              "      <td>-0.862574</td>\n",
              "      <td>-0.268891</td>\n",
              "      <td>-0.620369</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.230466</td>\n",
              "      <td>1.115425</td>\n",
              "      <td>-0.037654</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.045419</td>\n",
              "      <td>0.716228</td>\n",
              "      <td>4.074134</td>\n",
              "      <td>1.860037</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.159398</td>\n",
              "      <td>...</td>\n",
              "      <td>0.548128</td>\n",
              "      <td>-0.870957</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>0.307940</td>\n",
              "      <td>0.075593</td>\n",
              "      <td>0.602899</td>\n",
              "      <td>0.531052</td>\n",
              "      <td>0.244634</td>\n",
              "      <td>-1.569929</td>\n",
              "      <td>-0.087055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39639</th>\n",
              "      <td>0.284409</td>\n",
              "      <td>-0.425624</td>\n",
              "      <td>-0.005443</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.001345</td>\n",
              "      <td>-0.166227</td>\n",
              "      <td>0.961408</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>-0.029746</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.191633</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-0.027151</td>\n",
              "      <td>-0.003726</td>\n",
              "      <td>0.075593</td>\n",
              "      <td>-0.183488</td>\n",
              "      <td>-0.562389</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.308051</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39640</th>\n",
              "      <td>0.757438</td>\n",
              "      <td>-0.463832</td>\n",
              "      <td>0.042060</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.059998</td>\n",
              "      <td>-0.166227</td>\n",
              "      <td>0.961408</td>\n",
              "      <td>-0.185830</td>\n",
              "      <td>11.380666</td>\n",
              "      <td>-0.169056</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200878</td>\n",
              "      <td>0.573766</td>\n",
              "      <td>-0.228938</td>\n",
              "      <td>0.379039</td>\n",
              "      <td>0.420076</td>\n",
              "      <td>0.078641</td>\n",
              "      <td>0.054424</td>\n",
              "      <td>3.498112</td>\n",
              "      <td>-0.751322</td>\n",
              "      <td>3.729377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39641</th>\n",
              "      <td>-0.188620</td>\n",
              "      <td>-0.221849</td>\n",
              "      <td>-0.009049</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.013798</td>\n",
              "      <td>1.157456</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>0.897276</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.626102</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.445296</td>\n",
              "      <td>0.573766</td>\n",
              "      <td>-1.036087</td>\n",
              "      <td>-0.758776</td>\n",
              "      <td>-0.957859</td>\n",
              "      <td>-0.620369</td>\n",
              "      <td>0.531052</td>\n",
              "      <td>0.244634</td>\n",
              "      <td>-1.569929</td>\n",
              "      <td>-0.087055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39642</th>\n",
              "      <td>-2.080734</td>\n",
              "      <td>0.287589</td>\n",
              "      <td>-0.002477</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>-0.077982</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.505485</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.118920</td>\n",
              "      <td>-0.461973</td>\n",
              "      <td>-1.036087</td>\n",
              "      <td>0.424962</td>\n",
              "      <td>0.075593</td>\n",
              "      <td>0.996092</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39643</th>\n",
              "      <td>-0.188620</td>\n",
              "      <td>-0.826806</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.048082</td>\n",
              "      <td>-0.872192</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>-0.546866</td>\n",
              "      <td>0.182608</td>\n",
              "      <td>-0.091072</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.018606</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-1.036087</td>\n",
              "      <td>0.466031</td>\n",
              "      <td>1.109044</td>\n",
              "      <td>-0.969874</td>\n",
              "      <td>0.157226</td>\n",
              "      <td>0.672723</td>\n",
              "      <td>-0.927885</td>\n",
              "      <td>0.415107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39644 rows × 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e9d6844-1e40-449e-9e35-c87d1e30247a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3e9d6844-1e40-449e-9e35-c87d1e30247a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3e9d6844-1e40-449e-9e35-c87d1e30247a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
              "0            0.757438         -0.695202         0.032771          0.000675   \n",
              "1           -0.661648         -0.618786         0.016056          0.000675   \n",
              "2           -0.661648         -0.712183         0.007644          0.000675   \n",
              "3           -0.661648         -0.032932        -0.012619          0.000675   \n",
              "4            1.230466          1.115425        -0.037654          0.000675   \n",
              "...               ...               ...              ...               ...   \n",
              "39639        0.284409         -0.425624        -0.005443          0.000675   \n",
              "39640        0.757438         -0.463832         0.042060          0.000675   \n",
              "39641       -0.188620         -0.221849        -0.009049          0.000675   \n",
              "39642       -2.080734          0.287589        -0.002477          0.000675   \n",
              "39643       -0.188620         -0.826806         0.043676          0.000675   \n",
              "\n",
              "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
              "0                      0.038657  -0.607455       -0.335562 -0.426520   \n",
              "1                      0.031478  -0.695700       -0.594956 -0.426520   \n",
              "2                     -0.007752  -0.695700       -0.594956 -0.426520   \n",
              "3                     -0.007210  -0.166227       -0.854350 -0.426520   \n",
              "4                     -0.045419   0.716228        4.074134  1.860037   \n",
              "...                         ...        ...             ...       ...   \n",
              "39639                 -0.001345  -0.166227        0.961408 -0.426520   \n",
              "39640                  0.059998  -0.166227        0.961408 -0.185830   \n",
              "39641                 -0.013798   1.157456       -0.594956  0.897276   \n",
              "39642                  0.001067  -0.077982       -0.594956 -0.426520   \n",
              "39643                  0.048082  -0.872192       -0.594956 -0.546866   \n",
              "\n",
              "       num_videos  average_token_length  ...  avg_positive_polarity  \\\n",
              "0       -0.304264              0.156472  ...               0.237334   \n",
              "1       -0.304264              0.432833  ...              -0.640032   \n",
              "2       -0.304264             -0.183412  ...               1.358384   \n",
              "3       -0.304264             -0.169756  ...               0.307438   \n",
              "4       -0.304264              0.159398  ...               0.548128   \n",
              "...           ...                   ...  ...                    ...   \n",
              "39639   -0.060828             -0.029746  ...              -0.191633   \n",
              "39640   11.380666             -0.169056  ...               0.200878   \n",
              "39641   -0.060828              0.626102  ...              -0.445296   \n",
              "39642   -0.304264              0.505485  ...              -1.118920   \n",
              "39643    0.182608             -0.091072  ...              -1.018606   \n",
              "\n",
              "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
              "0                   0.063864              -0.228938              -0.708361   \n",
              "1                  -0.870957              -0.228938               1.102160   \n",
              "2                   0.063864               0.981786              -1.621776   \n",
              "3                   0.573766               0.174637              -0.862574   \n",
              "4                  -0.870957               0.981786               0.307940   \n",
              "...                      ...                    ...                    ...   \n",
              "39639               0.063864              -0.027151              -0.003726   \n",
              "39640               0.573766              -0.228938               0.379039   \n",
              "39641               0.573766              -1.036087              -0.758776   \n",
              "39642              -0.461973              -1.036087               0.424962   \n",
              "39643               0.063864              -1.036087               0.466031   \n",
              "\n",
              "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
              "0                  -0.268891              -0.969874            0.671237   \n",
              "1                   1.367406               0.078641           -0.870796   \n",
              "2                  -0.957859              -0.270864           -0.870796   \n",
              "3                  -0.268891              -0.620369           -0.870796   \n",
              "4                   0.075593               0.602899            0.531052   \n",
              "...                      ...                    ...                 ...   \n",
              "39639               0.075593              -0.183488           -0.562389   \n",
              "39640               0.420076               0.078641            0.054424   \n",
              "39641              -0.957859              -0.620369            0.531052   \n",
              "39642               0.075593               0.996092           -0.870796   \n",
              "39643               1.109044              -0.969874            0.157226   \n",
              "\n",
              "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
              "0                     -0.975420               -1.810696   \n",
              "1                     -0.269073                0.837738   \n",
              "2                     -0.269073                0.837738   \n",
              "3                     -0.269073                0.837738   \n",
              "4                      0.244634               -1.569929   \n",
              "...                         ...                     ...   \n",
              "39639                 -0.269073                0.308051   \n",
              "39640                  3.498112               -0.751322   \n",
              "39641                  0.244634               -1.569929   \n",
              "39642                 -0.269073                0.837738   \n",
              "39643                  0.672723               -0.927885   \n",
              "\n",
              "       abs_title_sentiment_polarity  \n",
              "0                          0.138918  \n",
              "1                         -0.689649  \n",
              "2                         -0.689649  \n",
              "3                         -0.689649  \n",
              "4                         -0.087055  \n",
              "...                             ...  \n",
              "39639                     -0.689649  \n",
              "39640                      3.729377  \n",
              "39641                     -0.087055  \n",
              "39642                     -0.689649  \n",
              "39643                      0.415107  \n",
              "\n",
              "[39644 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)    #normalizziamo soltanto la X ovvero le feature predittive\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)\n",
        "\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0OHYYue6xiV"
      },
      "source": [
        "**Training**\n",
        "\n",
        "Le funzioni ```calcola_gradiente``` e ```calcola_gradiente_bias``` sono ausiliarie ed usate poi dalla funzione ```Regression_train```.\n",
        "Calcolano la derivata della funzione di Loss. Quella utilizzata è la Loss quadratica.\n",
        "\n",
        ">$Loss: (1/n) * Σ(y_{true} - y_{pred})^2$\n",
        "\n",
        "Da cui si possono ottenere le derivate parziali considerando $y_{pred} = XW + b$\n",
        "\n",
        ">$∂Loss/∂W = (2/n) * X^T[y_{true} - (XW + b)]$\n",
        "\n",
        ">$∂Loss/∂b = (2/n) * Σ[y_{true} - (XW + b)]$\n",
        "\n",
        "La Funzione Regression Train costituisce la fase di ***gradient descent*** in cui ad ogni iterazione si spostano pesi e bias di un passetto lungo il gradiente gradiente (o derivata parziale) di Loss rispetto ad essi ma in direzione negativa, nel tentativo di ottenere un valore di Loss sempre minore raggiungendo il minimo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFpdcdHEBQDJ"
      },
      "outputs": [],
      "source": [
        "def calcola_gradiente(X, Y, predictions):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "\n",
        "  for i in range(nFeatures):\n",
        "      gradiente = -(2/nCampioni) * (np.dot(X.T.values, (Y.values.flatten() - predictions)))\n",
        "\n",
        "  return gradiente\n",
        "\n",
        "\n",
        "def calcola_gradiente_bias(X, Y, predictions):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "\n",
        "  gradient_bias = -(2/nCampioni) * np.sum(Y.values.flatten() - predictions)\n",
        "  return gradient_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1FdthxJ6qu6"
      },
      "outputs": [],
      "source": [
        "def Regression_train(X, Y, epochs, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.random.random_sample(nFeatures)\n",
        "  bias = np.random.random_sample()\n",
        "  for _ in range(epochs):\n",
        "    predictions = np.dot(X,W) + bias\n",
        "    gradient = calcola_gradiente(X, Y, predictions)\n",
        "    gradient_bias = calcola_gradiente_bias(X, Y, predictions)\n",
        "    W -= learning_rate * gradient\n",
        "    bias -= learning_rate * gradient_bias\n",
        "  return W, bias\n",
        "\n",
        "def Regression_predict(X, W, bias):\n",
        "  return np.dot(X, W) + bias;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lADcupd-9oi6"
      },
      "source": [
        "A questo punto si effettua una Grid Search per il processo di validation. Gli iperparametri su cui è possibile intervenire nella regressione lineare senza regolarizzazione sono soltanto il numero di epoche ovvero di iterazioni del gradient descent e il learning rate ovvero la lunghezza del passo lungo la discesa che il gradient descent compie ad ogni iterazione.\n",
        "\n",
        "Nei casi a seguire, per motivi di tempi di calcolo, si è preferito evitare la Cross Validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muasuuhjMVHB",
        "outputId": "fc59c14a-e1ea-4f68-905f-734f59df2d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  [10, 0.1] \n",
            "\tcurrent_rmse = 7757.677666787902\n",
            "\tbest_rmse = 7757.677666787902\n",
            "iperparametri =  [10, 0.01] \n",
            "\tcurrent_rmse = 8281.265685474982\n",
            "\tbest_rmse = 7757.677666787902\n",
            "iperparametri =  [10, 0.001] \n",
            "\tcurrent_rmse = 8568.455971071042\n",
            "\tbest_rmse = 7757.677666787902\n",
            "iperparametri =  [100, 0.1] \n",
            "\tcurrent_rmse = 7734.24041031716\n",
            "\tbest_rmse = 7734.24041031716\n",
            "iperparametri =  [100, 0.01] \n",
            "\tcurrent_rmse = 7761.863678084561\n",
            "\tbest_rmse = 7734.24041031716\n",
            "iperparametri =  [100, 0.001] \n",
            "\tcurrent_rmse = 8284.288139803784\n",
            "\tbest_rmse = 7734.24041031716\n",
            "iperparametri =  [500, 0.1] \n",
            "\tcurrent_rmse = 7731.133970627118\n",
            "\tbest_rmse = 7731.133970627118\n",
            "iperparametri =  [500, 0.01] \n",
            "\tcurrent_rmse = 7741.133255499004\n",
            "\tbest_rmse = 7731.133970627118\n",
            "iperparametri =  [500, 0.001] \n",
            "\tcurrent_rmse = 7851.546990086698\n",
            "\tbest_rmse = 7731.133970627118\n",
            "iperparametri =  [1000, 0.1] \n",
            "\tcurrent_rmse = 7731.76316006939\n",
            "\tbest_rmse = 7731.133970627118\n",
            "iperparametri =  [1000, 0.01] \n",
            "\tcurrent_rmse = 7734.297750649359\n",
            "\tbest_rmse = 7731.133970627118\n",
            "iperparametri =  [1000, 0.001] \n",
            "\tcurrent_rmse = 7762.307159454624\n",
            "\tbest_rmse = 7731.133970627118\n",
            "======================================================================================================\n",
            "Migliori_iperparametri: e= 500 lr= 0.1\n",
            "Miglior rmse sul validation set: 7731.133970627118\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "epochses = [10, 100, 500, 1000]             #valori di numero di epoche testati\n",
        "rates = [10**i for i in range(-1, -4, -1)]  #valori di learning rate testati\n",
        "\n",
        "best_rmse = float('inf')\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "\n",
        "    W, bias = Regression_train(train_X, train_Y, e, lr)\n",
        "    predictions = Regression_predict(validation_X, W, bias)\n",
        "\n",
        "    current_rmse = mean_squared_error(validation_Y, predictions, squared=False)\n",
        "\n",
        "    if( current_rmse < best_rmse):\n",
        "      best_rmse = current_rmse\n",
        "      best_e_r, best_lr_r = e, lr\n",
        "    print('iperparametri = ', [e, lr],'\\n\\tcurrent_rmse = '+str(current_rmse)+ '\\n\\tbest_rmse = '+str(best_rmse))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('Migliori_iperparametri: e=',best_e_r, 'lr=',best_lr_r)\n",
        "print('Miglior rmse sul validation set:', best_rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgf_0Tim_MCv"
      },
      "source": [
        "**Considerazioni sulla Validation**\n",
        "\n",
        "Osserviamo i risultati ottenuti in relazione al variare del numero massimo di iterazioni del gradient_descent. Anche qui si osserva un **andamento ad U** dell'errore (in questo caso mostrato chiaramente dall'rmse). Anche questa volta non ci aspettavamo nulla di diveso perchè aumentare le iterazioni porta la rappresentazione ad essere sempre più vicina ai dati di training ed esclude sempre più rappresentazioni.\n",
        "Oltre il valore di 500, il modello infatti va in overfitting, aumentando l'errore sul validation set, non studiato in fase di learning.\n",
        "\n",
        "Osserviamo invece l'impatto di variazioni del learning_rate. Diminuirne il valore sembra aumentare l'errore. Questo potrebbe indicare che valori di learning rate minori portano il gradient descent a convergere troppo lentamente e a non raggiungere l'ottimo entro il numero massimo di iterazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeAu98MT66S8",
        "outputId": "161fef73-9804-416b-de42-fd3817d6cd71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rmse sul test set con e= 500 lr= 0.1 :\n",
            " 6877.960398498197\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "test_W, test_bias = Regression_train(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y], axis=0), best_e_r, best_lr_r)\n",
        "test_predictions = Regression_predict(test_X, test_W, test_bias)\n",
        "test_rmse = mean_squared_error(test_Y, test_predictions, squared = False)\n",
        "\n",
        "print('Rmse sul test set con e=', best_e_r, 'lr=', best_lr_r, ':\\n', test_rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUtrLBInD3C2"
      },
      "source": [
        "Il risultato ottenuto sul Test Set è addirittura migliore di quello ottenuto sul Validaton Set. Questo può essere spiegato dal fatto che prima di essere eseguito sul Test Set, l'algoritmo, con gli iperparametri ottimali trovati sul Validation, è stato riallenato sull'unione del Training Set e del Validation Set, avendo più campioni a disposizione per trovare quindi un vettore dei pesi e un bias ancora migliore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZgDK7_BXRG1",
        "outputId": "1111a171-ce08-4c33-ad09-b2cced9608d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn_rmse = 6875.090034990688\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y], axis=0),)\n",
        "\n",
        "sklearn_predictions = model.predict(test_X)\n",
        "sklearn_rmse = mean_squared_error(test_Y, sklearn_predictions, squared = False)\n",
        "\n",
        "print('sklearn_rmse = ' + str(sklearn_rmse))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVQCsaT4FGwU"
      },
      "source": [
        "Il risultato ottenuto dall'implementazione fornita da scikit-learn è di poco inferiore al nostro. Questa differenza potrebbe essere dovuta al fatto che questa implementazione di LinearRegression della libreria sklearn usi la normal equation per trovare il vettore dei pesi migliore e non il gradient descent.\n",
        ">$W = (X^T * X)^{-1} * X^T * Y$\n",
        "\n",
        " Questo oltre a rendere l'algoritmo più veloce lo rende anche più preciso, ma comunque non molto più dell' implementazione proposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5gOuPcOuoWm"
      },
      "source": [
        "#Linear Regression as Classification senza Regolarizzazione\n",
        "\n",
        "Nei problemi di classificazione usando regressione lineare (a prescindere dalla regolarizzazione usata) è stato scelto di discretizzare soltanto la feature target e non quelle predittive, che sono state invece normalizzare come nei problemi di regressione. Questo perchè il funzionamento di base di questi algoritmi è lo stesso: prima si cerca una realzione lineare che interpoli le feature del dataset e che predica un valore continuo della colonna target, poi si introduce una  threshold che discretizzi questo risultato.\n",
        "\n",
        "Collassare la natura continua dei predittori in categorie discrete comporterebbe una perdita di informazioni che potrebbe distruggere una possibile relazione lineare esistente tra le feature. O comunque peggiorarle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98GC8JihuuDU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "Y_binary = Y_binary.flatten()\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=69)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e_BkN9aGYpX"
      },
      "source": [
        "**Considerazioni sul Training**\n",
        "\n",
        "Per i problemi di classificazione, se si vuole usare una threshold come nel nostro caso del tipo\n",
        "\n",
        ">$ h_w(x) =\n",
        "\\begin{cases}\n",
        "  1 : w*x > 0 \\\\\n",
        "  0 : altrimenti\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "non si può usare gradient descent, perchè la loss avrebbe solo valori +1 o 0 -1 essendo i valori di Y e delle predizioni solo 1 o 0. E quindi non sarebbe una funzione derivabile. Il gradiente non sarebbe quindi definito.\n",
        "\n",
        "Al suo posto si usa la perceptron rule:\n",
        ">$W_i ← W_i + α(Y−h_w(x))x_i$\n",
        "\n",
        "Altra cosa che cambia rispetto al problema di regressione è l'aggiunta di una riga alla funzione predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-jLSQqDwYGh"
      },
      "outputs": [],
      "source": [
        "def classification_train(X, Y, epochs, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.zeros(nFeatures)\n",
        "  bias = 0\n",
        "  for _ in range(epochs):\n",
        "    predictions = classification_predict(X, W, bias)\n",
        "    W += learning_rate * np.dot((Y-predictions), X)   #implementazione della perceptron updates rule\n",
        "    bias += learning_rate * np.sum((Y-predictions))\n",
        "  return W, bias\n",
        "\n",
        "def classification_predict(X, W, bias):\n",
        "  stime = np.dot(X, W) + bias\n",
        "  predictions = np.where(stime > 0, 1, 0)   #implementazione della threshold\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCkuuZ7JLGT"
      },
      "source": [
        "Il processo di validation è lo stesso del problema di regressione. Non ci si deve aspettare che i parametri migliori restino gli stessi perchè il processo di training è differente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzrw69-rwYGi",
        "outputId": "6ee7b7b3-b211-49d0-8f67-a3ecb9c5ef55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 \n",
            "\tcurrent_acc = 0.5988900100908173\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  10 0.01 \n",
            "\tcurrent_acc = 0.5988900100908173\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  10 0.001 \n",
            "\tcurrent_acc = 0.5988900100908173\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  10 0.0001 \n",
            "\tcurrent_acc = 0.5988900100908173\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  100 0.1 \n",
            "\tcurrent_acc = 0.5696266397578204\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  100 0.01 \n",
            "\tcurrent_acc = 0.5696266397578204\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  100 0.001 \n",
            "\tcurrent_acc = 0.5696266397578204\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  100 0.0001 \n",
            "\tcurrent_acc = 0.5696266397578204\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  500 0.1 \n",
            "\tcurrent_acc = 0.5693743693239153\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  500 0.01 \n",
            "\tcurrent_acc = 0.5693743693239153\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  500 0.001 \n",
            "\tcurrent_acc = 0.5693743693239153\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  500 0.0001 \n",
            "\tcurrent_acc = 0.5693743693239153\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  1000 0.1 \n",
            "\tcurrent_acc = 0.5842583249243188\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  1000 0.01 \n",
            "\tcurrent_acc = 0.5842583249243188\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  1000 0.001 \n",
            "\tcurrent_acc = 0.5842583249243188\n",
            "\tbest_acc = 0.5988900100908173\n",
            "iperparametri =  1000 0.0001 \n",
            "\tcurrent_acc = 0.5842583249243188\n",
            "\tbest_acc = 0.5988900100908173\n",
            "======================================================================================================\n",
            "Migliori_iperparametri: e= 10 lr= 0.0001\n",
            "Miglior accuracy sul validation set: 0.5988900100908173\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochses = [10, 100, 500, 1000]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "\n",
        "    W, bias = classification_train(train_X, train_Y, e, lr)\n",
        "    predictions = classification_predict(validation_X, W, bias)\n",
        "\n",
        "    current_acc = accuracy_score(validation_Y, predictions)\n",
        "\n",
        "    if( current_acc >= best_acc):\n",
        "      best_acc = current_acc\n",
        "      best_e_c, best_lr_c = e, lr\n",
        "    print('iperparametri = ', e, lr,'\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('Migliori_iperparametri: e=',best_e_c, 'lr=',best_lr_c)\n",
        "print('Miglior accuracy sul validation set:', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cMH4zkSF90v"
      },
      "source": [
        "il processo di validation seleziona un valore molto basso di iterazioni. Il che vuol dire che il perceptron update converge velocemente ad un valore abbastanza basso di errore, e continuare con le iterazioni non aumenta così tanto l'accuracy da compensare il rischio di overfitting. Per questo all'aumentare delle iterazioni, le performance scendono a parità di learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBUqecoJwYGj",
        "outputId": "8e83dedf-bf87-40e9-e3fa-d763d8878efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy sul test set con e= 10 lr= 0.0001 :\n",
            " 0.6156368221941992\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias = classification_train(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0), best_e_c, best_lr_c)\n",
        "test_predictions = classification_predict(test_X, test_W, test_bias)\n",
        "test_acc = accuracy_score(test_Y, test_predictions)\n",
        "\n",
        "print('Accuracy sul test set con e=', best_e_c, 'lr=', best_lr_c, ':\\n', test_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akAcgSpjwYGj",
        "outputId": "2abcdc01-e146-46b7-a758-4b8e617a1c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.639344262295082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "classifier = SGDClassifier(alpha=0, eta0=best_lr_c, learning_rate='constant', max_iter=best_e_c)\n",
        "\n",
        "classifier.fit(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0))\n",
        "\n",
        "sklearn_predictions = classifier.predict(test_X)\n",
        "\n",
        "sklearn_acc = accuracy_score(test_Y, sklearn_predictions)\n",
        "print(sklearn_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHlTFmCpogsv"
      },
      "source": [
        "# Linear Regression con regolarizzazione Ridge\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della regressione senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "49QTxFwwopy3",
        "outputId": "f990a326-a586-4f7c-f142-9a22c7883b1d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-65a912f7-fdc3-4b1d-8373-658140319fea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>num_hrefs</th>\n",
              "      <th>num_self_hrefs</th>\n",
              "      <th>num_imgs</th>\n",
              "      <th>num_videos</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>...</th>\n",
              "      <th>avg_positive_polarity</th>\n",
              "      <th>min_positive_polarity</th>\n",
              "      <th>max_positive_polarity</th>\n",
              "      <th>avg_negative_polarity</th>\n",
              "      <th>min_negative_polarity</th>\n",
              "      <th>max_negative_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "      <th>abs_title_subjectivity</th>\n",
              "      <th>abs_title_sentiment_polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9946</th>\n",
              "      <td>0.284409</td>\n",
              "      <td>-0.126329</td>\n",
              "      <td>-0.011529</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.008332</td>\n",
              "      <td>-0.342718</td>\n",
              "      <td>0.442620</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>-0.384199</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.174865</td>\n",
              "      <td>-0.637251</td>\n",
              "      <td>0.174637</td>\n",
              "      <td>0.259425</td>\n",
              "      <td>0.420076</td>\n",
              "      <td>0.078641</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>0.107646</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.247747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21688</th>\n",
              "      <td>-0.661648</td>\n",
              "      <td>-0.729164</td>\n",
              "      <td>0.026182</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.017415</td>\n",
              "      <td>-0.607455</td>\n",
              "      <td>-0.594956</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>0.182608</td>\n",
              "      <td>0.149972</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.142181</td>\n",
              "      <td>-0.637251</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>0.020197</td>\n",
              "      <td>0.649732</td>\n",
              "      <td>-0.183488</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18534</th>\n",
              "      <td>0.284409</td>\n",
              "      <td>-0.856524</td>\n",
              "      <td>0.037025</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.017001</td>\n",
              "      <td>-0.607455</td>\n",
              "      <td>-0.076168</td>\n",
              "      <td>-0.546866</td>\n",
              "      <td>0.426044</td>\n",
              "      <td>-0.483629</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.507617</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-1.036087</td>\n",
              "      <td>-1.621776</td>\n",
              "      <td>-0.613375</td>\n",
              "      <td>-0.969874</td>\n",
              "      <td>-0.562389</td>\n",
              "      <td>-0.834150</td>\n",
              "      <td>0.308051</td>\n",
              "      <td>-0.026795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13511</th>\n",
              "      <td>1.703495</td>\n",
              "      <td>0.253626</td>\n",
              "      <td>-0.026769</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.026992</td>\n",
              "      <td>0.539737</td>\n",
              "      <td>-0.076168</td>\n",
              "      <td>0.175205</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>-0.268731</td>\n",
              "      <td>...</td>\n",
              "      <td>0.570029</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>0.141788</td>\n",
              "      <td>-0.613375</td>\n",
              "      <td>0.602899</td>\n",
              "      <td>0.832449</td>\n",
              "      <td>0.647038</td>\n",
              "      <td>-1.533814</td>\n",
              "      <td>0.384977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22186</th>\n",
              "      <td>-1.134677</td>\n",
              "      <td>-0.487181</td>\n",
              "      <td>0.024872</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.045630</td>\n",
              "      <td>-0.783946</td>\n",
              "      <td>-0.335562</td>\n",
              "      <td>-0.546866</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.011985</td>\n",
              "      <td>...</td>\n",
              "      <td>1.256795</td>\n",
              "      <td>-0.870957</td>\n",
              "      <td>0.376424</td>\n",
              "      <td>0.688948</td>\n",
              "      <td>0.764560</td>\n",
              "      <td>0.078641</td>\n",
              "      <td>0.671237</td>\n",
              "      <td>1.614519</td>\n",
              "      <td>-1.810696</td>\n",
              "      <td>1.519864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29133</th>\n",
              "      <td>0.284409</td>\n",
              "      <td>1.917790</td>\n",
              "      <td>-0.032444</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.003123</td>\n",
              "      <td>0.098509</td>\n",
              "      <td>-0.854350</td>\n",
              "      <td>-0.185830</td>\n",
              "      <td>0.182608</td>\n",
              "      <td>0.050681</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.214714</td>\n",
              "      <td>-0.870957</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>0.473487</td>\n",
              "      <td>-1.646826</td>\n",
              "      <td>0.865027</td>\n",
              "      <td>1.442253</td>\n",
              "      <td>1.614519</td>\n",
              "      <td>-0.486479</td>\n",
              "      <td>1.519864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1894</th>\n",
              "      <td>-1.607706</td>\n",
              "      <td>-0.020197</td>\n",
              "      <td>-0.018573</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.001965</td>\n",
              "      <td>-0.342718</td>\n",
              "      <td>-0.854350</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.056447</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025432</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-0.228938</td>\n",
              "      <td>0.685672</td>\n",
              "      <td>0.075593</td>\n",
              "      <td>0.602899</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28559</th>\n",
              "      <td>0.757438</td>\n",
              "      <td>-0.243076</td>\n",
              "      <td>0.004598</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.018938</td>\n",
              "      <td>-0.254473</td>\n",
              "      <td>0.183226</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.060828</td>\n",
              "      <td>0.499367</td>\n",
              "      <td>...</td>\n",
              "      <td>0.505044</td>\n",
              "      <td>0.573766</td>\n",
              "      <td>0.174637</td>\n",
              "      <td>-0.326217</td>\n",
              "      <td>0.075593</td>\n",
              "      <td>0.078641</td>\n",
              "      <td>1.372161</td>\n",
              "      <td>0.929577</td>\n",
              "      <td>-0.606862</td>\n",
              "      <td>0.716404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12095</th>\n",
              "      <td>-0.188620</td>\n",
              "      <td>-0.569965</td>\n",
              "      <td>0.023789</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.016443</td>\n",
              "      <td>-0.430964</td>\n",
              "      <td>0.702014</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.091970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.244625</td>\n",
              "      <td>-0.870957</td>\n",
              "      <td>0.981786</td>\n",
              "      <td>0.270299</td>\n",
              "      <td>0.936802</td>\n",
              "      <td>-0.969874</td>\n",
              "      <td>-0.665191</td>\n",
              "      <td>-0.143500</td>\n",
              "      <td>0.484614</td>\n",
              "      <td>-0.542349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27764</th>\n",
              "      <td>2.176524</td>\n",
              "      <td>-0.289774</td>\n",
              "      <td>0.004735</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>-0.004574</td>\n",
              "      <td>0.627983</td>\n",
              "      <td>-0.335562</td>\n",
              "      <td>-0.426520</td>\n",
              "      <td>-0.304264</td>\n",
              "      <td>0.119067</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030403</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-0.632512</td>\n",
              "      <td>-0.497716</td>\n",
              "      <td>-0.613375</td>\n",
              "      <td>0.378217</td>\n",
              "      <td>-0.870796</td>\n",
              "      <td>-0.269073</td>\n",
              "      <td>0.837738</td>\n",
              "      <td>-0.689649</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3965 rows × 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65a912f7-fdc3-4b1d-8373-658140319fea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65a912f7-fdc3-4b1d-8373-658140319fea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65a912f7-fdc3-4b1d-8373-658140319fea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
              "9946         0.284409         -0.126329        -0.011529          0.000675   \n",
              "21688       -0.661648         -0.729164         0.026182          0.000675   \n",
              "18534        0.284409         -0.856524         0.037025          0.000675   \n",
              "13511        1.703495          0.253626        -0.026769          0.000675   \n",
              "22186       -1.134677         -0.487181         0.024872          0.000675   \n",
              "...               ...               ...              ...               ...   \n",
              "29133        0.284409          1.917790        -0.032444          0.000675   \n",
              "1894        -1.607706         -0.020197        -0.018573          0.000675   \n",
              "28559        0.757438         -0.243076         0.004598          0.000675   \n",
              "12095       -0.188620         -0.569965         0.023789          0.000675   \n",
              "27764        2.176524         -0.289774         0.004735          0.000675   \n",
              "\n",
              "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
              "9946                  -0.008332  -0.342718        0.442620 -0.426520   \n",
              "21688                  0.017415  -0.607455       -0.594956 -0.426520   \n",
              "18534                  0.017001  -0.607455       -0.076168 -0.546866   \n",
              "13511                 -0.026992   0.539737       -0.076168  0.175205   \n",
              "22186                  0.045630  -0.783946       -0.335562 -0.546866   \n",
              "...                         ...        ...             ...       ...   \n",
              "29133                 -0.003123   0.098509       -0.854350 -0.185830   \n",
              "1894                  -0.001965  -0.342718       -0.854350 -0.426520   \n",
              "28559                  0.018938  -0.254473        0.183226 -0.426520   \n",
              "12095                  0.016443  -0.430964        0.702014 -0.426520   \n",
              "27764                 -0.004574   0.627983       -0.335562 -0.426520   \n",
              "\n",
              "       num_videos  average_token_length  ...  avg_positive_polarity  \\\n",
              "9946    -0.304264             -0.384199  ...              -0.174865   \n",
              "21688    0.182608              0.149972  ...              -0.142181   \n",
              "18534    0.426044             -0.483629  ...              -0.507617   \n",
              "13511   -0.060828             -0.268731  ...               0.570029   \n",
              "22186   -0.060828              0.011985  ...               1.256795   \n",
              "...           ...                   ...  ...                    ...   \n",
              "29133    0.182608              0.050681  ...              -0.214714   \n",
              "1894    -0.060828              0.056447  ...              -0.025432   \n",
              "28559   -0.060828              0.499367  ...               0.505044   \n",
              "12095   -0.304264              0.091970  ...              -0.244625   \n",
              "27764   -0.304264              0.119067  ...               0.030403   \n",
              "\n",
              "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
              "9946               -0.637251               0.174637               0.259425   \n",
              "21688              -0.637251               0.981786               0.020197   \n",
              "18534               0.063864              -1.036087              -1.621776   \n",
              "13511               0.063864               0.981786               0.141788   \n",
              "22186              -0.870957               0.376424               0.688948   \n",
              "...                      ...                    ...                    ...   \n",
              "29133              -0.870957               0.981786               0.473487   \n",
              "1894                0.063864              -0.228938               0.685672   \n",
              "28559               0.573766               0.174637              -0.326217   \n",
              "12095              -0.870957               0.981786               0.270299   \n",
              "27764               0.063864              -0.632512              -0.497716   \n",
              "\n",
              "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
              "9946                0.420076               0.078641           -0.870796   \n",
              "21688               0.649732              -0.183488           -0.870796   \n",
              "18534              -0.613375              -0.969874           -0.562389   \n",
              "13511              -0.613375               0.602899            0.832449   \n",
              "22186               0.764560               0.078641            0.671237   \n",
              "...                      ...                    ...                 ...   \n",
              "29133              -1.646826               0.865027            1.442253   \n",
              "1894                0.075593               0.602899           -0.870796   \n",
              "28559               0.075593               0.078641            1.372161   \n",
              "12095               0.936802              -0.969874           -0.665191   \n",
              "27764              -0.613375               0.378217           -0.870796   \n",
              "\n",
              "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
              "9946                   0.107646                0.837738   \n",
              "21688                 -0.269073                0.837738   \n",
              "18534                 -0.834150                0.308051   \n",
              "13511                  0.647038               -1.533814   \n",
              "22186                  1.614519               -1.810696   \n",
              "...                         ...                     ...   \n",
              "29133                  1.614519               -0.486479   \n",
              "1894                  -0.269073                0.837738   \n",
              "28559                  0.929577               -0.606862   \n",
              "12095                 -0.143500                0.484614   \n",
              "27764                 -0.269073                0.837738   \n",
              "\n",
              "       abs_title_sentiment_polarity  \n",
              "9946                      -0.247747  \n",
              "21688                     -0.689649  \n",
              "18534                     -0.026795  \n",
              "13511                      0.384977  \n",
              "22186                      1.519864  \n",
              "...                             ...  \n",
              "29133                      1.519864  \n",
              "1894                      -0.689649  \n",
              "28559                      0.716404  \n",
              "12095                     -0.542349  \n",
              "27764                     -0.689649  \n",
              "\n",
              "[3965 rows x 58 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)\n",
        "test_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p__nbPo_Ma7y"
      },
      "source": [
        "Il calcolo del gradiente deve tener conto ora del termine di regolarizzazione. In questo caso, il termine è rappresentato da\n",
        "> $2 * lambda * W$\n",
        "\n",
        "lambda rappresenta l'entità della penalizzazione inferta dalla regolarizzazione. Maggiore lambda, maggiore la penalizzazione dell'overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxL4xIDTEHHD"
      },
      "outputs": [],
      "source": [
        "def ridge_calcola_gradiente(X, Y, W, predictions, lmbda):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  for i in range(nFeatures):\n",
        "      gradiente[i] = -(2/nCampioni) * (np.dot(X.values[:, i], (Y.values.flatten() - predictions))) + 2 * lmbda * W[i]\n",
        "\n",
        "  return gradiente\n",
        "\n",
        "\n",
        "def calcola_gradiente_bias(X, Y, predictions):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  gradient_bias = -(2/nCampioni) * np.sum(Y.values.flatten() - predictions)\n",
        "  return gradient_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcbPm9vNEHHH"
      },
      "outputs": [],
      "source": [
        "def ridgeRegression_train(X, Y, epochs, learning_rate, lmbda):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.random.random_sample(nFeatures)\n",
        "  bias = np.random.random_sample()\n",
        "  for _ in range(epochs):\n",
        "    predictions = np.dot(X,W) + bias\n",
        "    gradient_W = ridge_calcola_gradiente(X, Y, W, predictions, lmbda)\n",
        "    gradient_bias = calcola_gradiente_bias(X, Y, predictions)\n",
        "    W -= learning_rate * gradient_W\n",
        "    bias -= learning_rate * gradient_bias\n",
        "  return W, bias\n",
        "\n",
        "def ridgeRegression_predict(X, W, bias):\n",
        "  return np.dot(X, W) + bias;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC-q0sY9zfh4",
        "outputId": "d7458a80-14b8-48ad-93d1-b321733f485f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 \n",
            "\tcurrent_rmse = 7757.614576900076\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7758.953672789826\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.1 0.5 \n",
            "\tcurrent_rmse = 7761.460108715975\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7764.620776967443\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.1 0.9 \n",
            "\tcurrent_rmse = 7768.1045451782\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.01 0.1 \n",
            "\tcurrent_rmse = 8281.862301344578\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 8282.875619515324\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.01 0.5 \n",
            "\tcurrent_rmse = 8284.23252810343\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 8285.217387358209\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.01 0.9 \n",
            "\tcurrent_rmse = 8286.562170921174\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.001 0.1 \n",
            "\tcurrent_rmse = 8568.364572480717\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8568.352016881921\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.001 0.5 \n",
            "\tcurrent_rmse = 8568.640602346315\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8568.77466024283\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.001 0.9 \n",
            "\tcurrent_rmse = 8568.512157319801\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.0001 0.1 \n",
            "\tcurrent_rmse = 8607.033156614263\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8607.212270913458\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.0001 0.5 \n",
            "\tcurrent_rmse = 8607.102084309094\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8607.211467257204\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  10 0.0001 0.9 \n",
            "\tcurrent_rmse = 8607.22683660942\n",
            "\tbest_rmse = 7757.614576900076\n",
            "iperparametri =  100 0.1 0.1 \n",
            "\tcurrent_rmse = 7740.367119574222\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7746.931174884433\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.1 0.5 \n",
            "\tcurrent_rmse = 7751.417095497989\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7755.581556130309\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.1 0.9 \n",
            "\tcurrent_rmse = 7759.612897725889\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.01 0.1 \n",
            "\tcurrent_rmse = 7762.001552777733\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 7763.535176314526\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.01 0.5 \n",
            "\tcurrent_rmse = 7766.1428791148755\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 7769.308340221913\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.01 0.9 \n",
            "\tcurrent_rmse = 7772.747202568567\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.001 0.1 \n",
            "\tcurrent_rmse = 8285.011069370017\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8286.25341618935\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.001 0.5 \n",
            "\tcurrent_rmse = 8287.475798694652\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8288.56580177308\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.001 0.9 \n",
            "\tcurrent_rmse = 8289.788954661602\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.0001 0.1 \n",
            "\tcurrent_rmse = 8568.323131822675\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8568.52631556088\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.0001 0.5 \n",
            "\tcurrent_rmse = 8568.653077485904\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8568.79220061765\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  100 0.0001 0.9 \n",
            "\tcurrent_rmse = 8568.721165064326\n",
            "\tbest_rmse = 7740.367119574222\n",
            "iperparametri =  500 0.1 0.1 \n",
            "\tcurrent_rmse = 7739.861289452264\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7746.927047260042\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.1 0.5 \n",
            "\tcurrent_rmse = 7751.417060743147\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7755.581555860664\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.1 0.9 \n",
            "\tcurrent_rmse = 7759.612897725292\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.01 0.1 \n",
            "\tcurrent_rmse = 7742.715181418532\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 7747.137260123273\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.01 0.5 \n",
            "\tcurrent_rmse = 7751.43633240347\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 7755.583295392579\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.01 0.9 \n",
            "\tcurrent_rmse = 7759.612823024429\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.001 0.1 \n",
            "\tcurrent_rmse = 7852.310927417556\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 7854.317402417126\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.001 0.5 \n",
            "\tcurrent_rmse = 7856.726007168607\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 7859.424129231121\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.001 0.9 \n",
            "\tcurrent_rmse = 7862.334037197533\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.0001 0.1 \n",
            "\tcurrent_rmse = 8423.025150085827\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8423.408392950809\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.0001 0.5 \n",
            "\tcurrent_rmse = 8423.823904962224\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8424.620683822322\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  500 0.0001 0.9 \n",
            "\tcurrent_rmse = 8425.039820358421\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.1 0.1 \n",
            "\tcurrent_rmse = 7739.861290685219\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7746.927047260042\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.1 0.5 \n",
            "\tcurrent_rmse = 7751.417060743146\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7755.581555860664\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.1 0.9 \n",
            "\tcurrent_rmse = 7759.612897725292\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.01 0.1 \n",
            "\tcurrent_rmse = 7740.394740412652\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 7746.932390175964\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.01 0.5 \n",
            "\tcurrent_rmse = 7751.41712548358\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 7755.58155673228\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.01 0.9 \n",
            "\tcurrent_rmse = 7759.612897727072\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.001 0.1 \n",
            "\tcurrent_rmse = 7762.475913521889\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 7764.039854714838\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.001 0.5 \n",
            "\tcurrent_rmse = 7766.648910108833\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 7769.809733267584\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.001 0.9 \n",
            "\tcurrent_rmse = 7773.253801751094\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.0001 0.1 \n",
            "\tcurrent_rmse = 8285.395513399835\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8286.482563509651\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.0001 0.5 \n",
            "\tcurrent_rmse = 8287.812183500193\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8288.900591648038\n",
            "\tbest_rmse = 7739.861289452264\n",
            "iperparametri =  1000 0.0001 0.9 \n",
            "\tcurrent_rmse = 8290.151875124777\n",
            "\tbest_rmse = 7739.861289452264\n",
            "======================================================================================================\n",
            "Migliori_iperparametri: e= 500 l= 0.1 lr= 0.1\n",
            "Miglior rmse sul validation set: 7739.861289452264\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "epochses = [10, 100, 500, 1000]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "lmbdas = [i*10**-1 for i in range(1, 10, 2)]\n",
        "best_rmse = float('inf')\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "    for l in lmbdas :\n",
        "      W, bias = ridgeRegression_train(train_X, train_Y, e, lr, l)\n",
        "      predictions = ridgeRegression_predict(validation_X, W, bias)\n",
        "      current_rmse = mean_squared_error(validation_Y, predictions, squared=False)\n",
        "\n",
        "      if( current_rmse < best_rmse):\n",
        "        best_rmse = current_rmse\n",
        "        best_e, best_lr, best_l = e, lr, l\n",
        "      print('iperparametri = ', e, lr, l, '\\n\\tcurrent_rmse = '+str(current_rmse)+ '\\n\\tbest_rmse = '+str(best_rmse))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('Migliori_iperparametri: e=',best_e, 'l=', best_l, 'lr=',best_lr)\n",
        "print('Miglior rmse sul validation set:', best_rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pJpwzd5RaLo"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "Valgono le osservazioni fatte nel caso senza regolarizzazione per il numero di iterazioni ed il learning rate. Ovvero: l'andamento dell'errore in funzione delle iterazioni è ad U, quindi oltre una soglia c'è overfitting; diminuire il learning rate fa rallentare la convergenza.\n",
        "\n",
        " Per quanto riguarda invece il peso della penalizzazione, si osserva che valori piccoli vengono prediletti. Questo potrebbe indicare che modelli complessi sono più adatti a descrivere il problema, che come osservato in precedenza ha una certa complessità (59 features di interesse).\n",
        "\n",
        "Penalizzare troppo la complessità porta il modello a non riuscire a trovare più una relazione lineare abbastanza buona per stimare la colonna 'shares'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5e6_PqrH2KJ",
        "outputId": "cbcadfa7-e931-422d-c68d-22e41319dbaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rmse sul test set con e= 500 l= 0.1 lr= 0.1 :\n",
            " 6873.572181610272\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias= ridgeRegression_train(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y], axis=0), best_e, best_lr , best_l)\n",
        "test_predictions = ridgeRegression_predict(test_X, test_W, test_bias)\n",
        "test_rmse_r = mean_squared_error(test_Y, test_predictions, squared = False)\n",
        "\n",
        "print('Rmse sul test set con e=', best_e, 'l=', best_l, 'lr=', best_lr, ':\\n', test_rmse_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XUN2x6WWHTd"
      },
      "source": [
        "Anche qui sul Test Set abbiamo ottenuto risultati migliori di quelli sul Validation Set, sintomo di assenza di Overfitting. Un altro motivo per cui può essere migliore è per l'ampliato insieme di training su cui si allena l'algoritmo prima di essere eseguito sul Test Set, che comprende anche l'ex Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5AVjNmxzUcd",
        "outputId": "9cb78612-858c-4b52-adee-6dd0c80f5ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn rmse: 6875.030139377504\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=best_l)\n",
        "\n",
        "ridge.fit(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y], axis=0))\n",
        "sklearn_predictions = ridge.predict(test_X[feature_names])\n",
        "sklearn_rmse_r = mean_squared_error(test_Y, sklearn_predictions, squared = False)\n",
        "print(\"Sklearn rmse:\", sklearn_rmse_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHlRfkL3IBt2"
      },
      "source": [
        "Sklearn commette un errore peggiore dell'implementazione proposta, seppur di poco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FrZViwe8tjj"
      },
      "source": [
        "#Linear Regression as Classification regolarizzazione Ridge\n",
        "\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della classificazione con regressione lineare senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_38UJ2sg81ls"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "Y_binary = Y_binary.flatten()\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hjdn7ovQuQ8"
      },
      "source": [
        "Anche qui va aggiunto il termine di regolarizzazione alla perceptron update rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PkpUb_B81lt"
      },
      "outputs": [],
      "source": [
        "def ridge_classification_train(X, Y, epochs,lmbda, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.zeros(nFeatures)\n",
        "  bias = 0\n",
        "  for _ in range(epochs):\n",
        "    predictions = classification_predict(X, W, bias)\n",
        "    W += learning_rate * np.dot((Y-predictions), X) + lmbda * W\n",
        "    bias += learning_rate * np.sum((Y-predictions))\n",
        "  return W, bias\n",
        "\n",
        "def classification_predict(X, W, bias):\n",
        "  stime = np.dot(X, W) + bias\n",
        "  predictions = np.where(stime > 0, 1, 0)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1qoA6ZK81lt",
        "outputId": "42dadd62-66d6-4e82-9aaf-bfcab5735b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6117558022199798\n",
            "iperparametri =  10 0.01 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6117558022199798\n",
            "iperparametri =  10 0.001 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6117558022199798\n",
            "iperparametri =  10 0.0001 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6117558022199798\n",
            "iperparametri =  10 0.1 0.2 \n",
            "\tcurrent_acc = 0.62058526740666\n",
            "\tbest_acc = 0.62058526740666\n",
            "iperparametri =  10 0.01 0.2 \n",
            "\tcurrent_acc = 0.62058526740666\n",
            "\tbest_acc = 0.62058526740666\n",
            "iperparametri =  10 0.001 0.2 \n",
            "\tcurrent_acc = 0.62058526740666\n",
            "\tbest_acc = 0.62058526740666\n",
            "iperparametri =  10 0.0001 0.2 \n",
            "\tcurrent_acc = 0.62058526740666\n",
            "\tbest_acc = 0.62058526740666\n",
            "iperparametri =  10 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6233602421796165\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6233602421796165\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6233602421796165\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6233602421796165\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.4 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.4 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.4 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.5 \n",
            "\tcurrent_acc = 0.615539858728557\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.5 \n",
            "\tcurrent_acc = 0.615539858728557\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.5 \n",
            "\tcurrent_acc = 0.615539858728557\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.5 \n",
            "\tcurrent_acc = 0.615539858728557\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6024217961654894\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6024217961654894\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6024217961654894\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6024217961654894\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.8 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.8 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.8 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.9 \n",
            "\tcurrent_acc = 0.6125126135216953\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.9 \n",
            "\tcurrent_acc = 0.6125126135216953\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.9 \n",
            "\tcurrent_acc = 0.6125126135216953\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6125126135216953\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.2 \n",
            "\tcurrent_acc = 0.5991422805247225\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6026740665993946\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5968718466195762\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5918264379414733\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6054490413723511\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.1 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.01 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  1000 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "======================================================================================================\n",
            "best_iperparametri =  10 0.0001 0.30000000000000004\n",
            "best = 0.6233602421796165\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochses = [10, 100, 500, 1000]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "lmbdas = [i*10**-1 for i in range(1, 10)]\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for l in lmbdas:\n",
        "    for lr in rates:\n",
        "\n",
        "      W, bias = ridge_classification_train(train_X, train_Y, e, l, lr)\n",
        "      predictions = classification_predict(validation_X, W, bias)\n",
        "\n",
        "      current_acc = accuracy_score(validation_Y, predictions)\n",
        "\n",
        "      if( current_acc >= best_acc):\n",
        "        best_acc = current_acc\n",
        "        best_e, best_lr, best_l = e, lr, l\n",
        "      print('iperparametri = ', e, lr, l, '\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_lr, best_l)\n",
        "print('best =', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CAugXRSJao0"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "Per il numero di iterazioni vale lo stesso discorso fatto nel caso della classificazione senza regolarizzazione. Notiamo infatti una certa velocità di convergenza e una tendenza a ridurre la capacità di generalizzazione già da 100 iterazioni.\n",
        "\n",
        "Proprio questa velocità di convergenza dei perceptron updates potrebbe essere il motivo della selezione di un valore di l (entità della regolarizzazione) più alto rispetto al caso della regressione (qui 0.3, lì 0.1): visto che inizia ad overfittare molto presto, una penalizzazione alla complessità del modello apporta un beneficio in termini di accuracy. In confronto il gradient descent non era così rapido e anzi selezionava anche un learning rate più alto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbwKv0qb81lt",
        "outputId": "1f15108b-bbd4-401e-aee3-03113dd08995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_acc = 0.6408575031525852\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias = ridge_classification_train(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0), best_e, best_l, best_lr)\n",
        "test_predictions = classification_predict(test_X, test_W, test_bias)\n",
        "test_acc_r = accuracy_score(test_Y, test_predictions)\n",
        "\n",
        "print('test_acc = ' + str(test_acc_r))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZU4MJUO81lu",
        "outputId": "f3bf7bf6-a42b-4a86-aba7-ab491e6e549f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6259773013871375\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "classifier = SGDClassifier( alpha=0.3, penalty='l2', eta0=0.0001, learning_rate='constant', max_iter=best_e)\n",
        "\n",
        "pipeline = Pipeline([('classifier', classifier)])\n",
        "\n",
        "pipeline.fit(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0))\n",
        "\n",
        "sklearn_predictions = pipeline.predict(test_X)\n",
        "\n",
        "sklearn_acc_r = accuracy_score(test_Y, sklearn_predictions)\n",
        "print(sklearn_acc_r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU5xKRuNMIMM"
      },
      "source": [
        "Sklearn ha un accuracy inferiore al modello proposto di circa 1.5% a parità di iperparametri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsTixhjd4I2g"
      },
      "source": [
        "# Linear Regression con regolarizzazione Lasso\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della regressione senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FvdZzwTUEgM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLaydfOHuyYR"
      },
      "outputs": [],
      "source": [
        "nFeatures = len(feature_names)\n",
        "W = np.random.random_sample(nFeatures)\n",
        "bias = np.random.random_sample()\n",
        "predictions = np.dot(test_X,W) + bias\n",
        "\n",
        "\n",
        "def costo(true_Y, pred_Y, lmbda):\n",
        "  n = true_Y.shape[0]\n",
        "  loss = (1/n) * np.sum(np.square(true_Y.values.flatten() - pred_Y.flatten())) + lmbda*np.sum(np.abs(W))\n",
        "  return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbQz3nQgQ4hQ"
      },
      "source": [
        "in questo caso il termine di regolarizzazione è il segno del peso, perchè la regolarizzazoine L1 aggiunge al costo della funzione la somma dei valori assoluti dei pesi. In questo modo penalizza la complessità del modello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_xSv1rvTOul"
      },
      "outputs": [],
      "source": [
        "def lasso_calcola_gradiente(X, Y, predictions, lmbda):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  for i in range(nFeatures):\n",
        "      gradiente[i] = -(2/nCampioni) * (np.dot(X.values[:, i], (Y.values.flatten() - predictions)) + (W[i]>0)*lmbda)\n",
        "  return gradiente\n",
        "\n",
        "\n",
        "def calcola_gradiente_bias(X, Y, predictions, lmbda):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  gradient_bias = -(2/nCampioni) * np.sum(Y.values.flatten() - predictions)\n",
        "  return gradient_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0GAKKLebk6J"
      },
      "outputs": [],
      "source": [
        "def lassoRegression_train(X, Y, epochs, lmbda, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.random.random_sample(nFeatures)\n",
        "  bias = np.random.random_sample()\n",
        "  for _ in range(epochs):\n",
        "    predictions = np.dot(X,W) + bias\n",
        "    gradient = lasso_calcola_gradiente(X, Y, predictions, lmbda)\n",
        "    gradient_bias = calcola_gradiente_bias(X, Y, predictions, lmbda)\n",
        "    W -= learning_rate * gradient\n",
        "    bias -= learning_rate * gradient_bias\n",
        "  return W, bias\n",
        "\n",
        "def lassoRegression_predict(X, W, bias):\n",
        "  return np.dot(X, W) + bias;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN2CJzWvI60c",
        "outputId": "e68c6b64-0bad-441a-cb15-6eec021632ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 \n",
            "\tcurrent_rmse = 7757.693411097132\n",
            "\tbest_rmse = 7757.693411097132\n",
            "iperparametri =  10 0.1 0.2 \n",
            "\tcurrent_rmse = 7757.69611842918\n",
            "\tbest_rmse = 7757.693411097132\n",
            "iperparametri =  10 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7757.674215149231\n",
            "\tbest_rmse = 7757.674215149231\n",
            "iperparametri =  10 0.1 0.4 \n",
            "\tcurrent_rmse = 7757.677147502737\n",
            "\tbest_rmse = 7757.674215149231\n",
            "iperparametri =  10 0.1 0.5 \n",
            "\tcurrent_rmse = 7757.687372357702\n",
            "\tbest_rmse = 7757.674215149231\n",
            "iperparametri =  10 0.1 0.6000000000000001 \n",
            "\tcurrent_rmse = 7757.666954160113\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7757.669121788862\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.1 0.8 \n",
            "\tcurrent_rmse = 7757.671930918443\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.1 0.9 \n",
            "\tcurrent_rmse = 7757.684195248458\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.1 \n",
            "\tcurrent_rmse = 8281.336695015982\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.2 \n",
            "\tcurrent_rmse = 8281.26084496421\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 8281.200965720876\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.4 \n",
            "\tcurrent_rmse = 8281.418869297435\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.5 \n",
            "\tcurrent_rmse = 8281.15915916341\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.6000000000000001 \n",
            "\tcurrent_rmse = 8281.210328827538\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 8281.25385138301\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.8 \n",
            "\tcurrent_rmse = 8281.14996937702\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.01 0.9 \n",
            "\tcurrent_rmse = 8281.11928689451\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.1 \n",
            "\tcurrent_rmse = 8568.342408951861\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.2 \n",
            "\tcurrent_rmse = 8568.547106546643\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8568.734497393229\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.4 \n",
            "\tcurrent_rmse = 8568.2851023078\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.5 \n",
            "\tcurrent_rmse = 8568.58740433236\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.6000000000000001 \n",
            "\tcurrent_rmse = 8568.26640982776\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8568.490891322048\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.8 \n",
            "\tcurrent_rmse = 8568.238276540722\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.001 0.9 \n",
            "\tcurrent_rmse = 8568.275742943553\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.1 \n",
            "\tcurrent_rmse = 8607.496551652142\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.2 \n",
            "\tcurrent_rmse = 8606.986121252627\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8607.052279328393\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.4 \n",
            "\tcurrent_rmse = 8607.26114325435\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.5 \n",
            "\tcurrent_rmse = 8607.383684830065\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.6000000000000001 \n",
            "\tcurrent_rmse = 8607.107693867445\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8607.315513394593\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.8 \n",
            "\tcurrent_rmse = 8607.233058235917\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  10 0.0001 0.9 \n",
            "\tcurrent_rmse = 8607.221054194708\n",
            "\tbest_rmse = 7757.666954160113\n",
            "iperparametri =  250 0.1 0.1 \n",
            "\tcurrent_rmse = 7730.4170620908335\n",
            "\tbest_rmse = 7730.4170620908335\n",
            "iperparametri =  250 0.1 0.2 \n",
            "\tcurrent_rmse = 7730.413265050685\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7730.416378064334\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.4 \n",
            "\tcurrent_rmse = 7730.416914908183\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.5 \n",
            "\tcurrent_rmse = 7730.416356095571\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.6000000000000001 \n",
            "\tcurrent_rmse = 7730.415079493191\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7730.414615818952\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.8 \n",
            "\tcurrent_rmse = 7730.414336923682\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.1 0.9 \n",
            "\tcurrent_rmse = 7730.415258928411\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.1 \n",
            "\tcurrent_rmse = 7746.567204092956\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.2 \n",
            "\tcurrent_rmse = 7746.560562403587\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 7746.559574073918\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.4 \n",
            "\tcurrent_rmse = 7746.550521371164\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.5 \n",
            "\tcurrent_rmse = 7746.575277194326\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.6000000000000001 \n",
            "\tcurrent_rmse = 7746.568223964555\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 7746.569219314677\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.8 \n",
            "\tcurrent_rmse = 7746.557105389252\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.01 0.9 \n",
            "\tcurrent_rmse = 7746.5770310391445\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.1 \n",
            "\tcurrent_rmse = 8032.24802246483\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.2 \n",
            "\tcurrent_rmse = 8032.192389315175\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8032.184190441446\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.4 \n",
            "\tcurrent_rmse = 8032.1628227658875\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.5 \n",
            "\tcurrent_rmse = 8032.229396945774\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.6000000000000001 \n",
            "\tcurrent_rmse = 8032.214303188867\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8032.1544529661005\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.8 \n",
            "\tcurrent_rmse = 8032.186833482511\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.001 0.9 \n",
            "\tcurrent_rmse = 8032.145467316136\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.1 \n",
            "\tcurrent_rmse = 8509.052200520135\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.2 \n",
            "\tcurrent_rmse = 8509.047453182182\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8509.03701765188\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.4 \n",
            "\tcurrent_rmse = 8509.509414105622\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.5 \n",
            "\tcurrent_rmse = 8509.376881886634\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.6000000000000001 \n",
            "\tcurrent_rmse = 8509.016850810884\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8509.099844267826\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.8 \n",
            "\tcurrent_rmse = 8509.121065060033\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  250 0.0001 0.9 \n",
            "\tcurrent_rmse = 8509.073587496292\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.1 \n",
            "\tcurrent_rmse = 7731.134268420954\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.2 \n",
            "\tcurrent_rmse = 7731.134100235963\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.30000000000000004 \n",
            "\tcurrent_rmse = 7731.134655419989\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.4 \n",
            "\tcurrent_rmse = 7731.133879699135\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.5 \n",
            "\tcurrent_rmse = 7731.133459639678\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.6000000000000001 \n",
            "\tcurrent_rmse = 7731.133680301439\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.7000000000000001 \n",
            "\tcurrent_rmse = 7731.13432388085\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.8 \n",
            "\tcurrent_rmse = 7731.133618470086\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.1 0.9 \n",
            "\tcurrent_rmse = 7731.133530036007\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.1 \n",
            "\tcurrent_rmse = 7741.145023837474\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.2 \n",
            "\tcurrent_rmse = 7741.148192482257\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.30000000000000004 \n",
            "\tcurrent_rmse = 7741.148370294154\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.4 \n",
            "\tcurrent_rmse = 7741.1408964866105\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.5 \n",
            "\tcurrent_rmse = 7741.137340522368\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.6000000000000001 \n",
            "\tcurrent_rmse = 7741.131243311298\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.7000000000000001 \n",
            "\tcurrent_rmse = 7741.151883354341\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.8 \n",
            "\tcurrent_rmse = 7741.143222121241\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.01 0.9 \n",
            "\tcurrent_rmse = 7741.128646697223\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.1 \n",
            "\tcurrent_rmse = 7851.567083043118\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.2 \n",
            "\tcurrent_rmse = 7851.521092870559\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.30000000000000004 \n",
            "\tcurrent_rmse = 7851.534483303739\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.4 \n",
            "\tcurrent_rmse = 7851.507224517614\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.5 \n",
            "\tcurrent_rmse = 7851.57212662139\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.6000000000000001 \n",
            "\tcurrent_rmse = 7851.563270449604\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.7000000000000001 \n",
            "\tcurrent_rmse = 7851.537166671837\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.8 \n",
            "\tcurrent_rmse = 7851.554636826851\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.001 0.9 \n",
            "\tcurrent_rmse = 7851.580445264528\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.1 \n",
            "\tcurrent_rmse = 8422.57163020934\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.2 \n",
            "\tcurrent_rmse = 8422.476687503628\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.30000000000000004 \n",
            "\tcurrent_rmse = 8422.614714681513\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.4 \n",
            "\tcurrent_rmse = 8422.441343181144\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.5 \n",
            "\tcurrent_rmse = 8422.55484320835\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.6000000000000001 \n",
            "\tcurrent_rmse = 8422.643976806723\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.7000000000000001 \n",
            "\tcurrent_rmse = 8422.712544687512\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.8 \n",
            "\tcurrent_rmse = 8422.51703288022\n",
            "\tbest_rmse = 7730.413265050685\n",
            "iperparametri =  500 0.0001 0.9 \n",
            "\tcurrent_rmse = 8422.523150564994\n",
            "\tbest_rmse = 7730.413265050685\n",
            "======================================================================================================\n",
            "best_iperparametri =  250 0.1 0.2\n",
            "best = 7730.413265050685\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "epochses = [10, 250, 500]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "lmbdas = [i*10**-1 for i in range(1,10)]\n",
        "best_rmse = float('inf')\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "    for l in lmbdas :\n",
        "      W, bias = lassoRegression_train(train_X, train_Y, e, l, lr)\n",
        "      predictions = lassoRegression_predict(validation_X, W, bias)\n",
        "      current_rmse = mean_squared_error(validation_Y, predictions, squared=False)\n",
        "\n",
        "      if( current_rmse < best_rmse):\n",
        "        best_rmse = current_rmse\n",
        "        best_e, best_lr, best_l = e, lr, l\n",
        "      print('iperparametri = ', e, lr, l, '\\n\\tcurrent_rmse = '+str(current_rmse)+ '\\n\\tbest_rmse = '+str(best_rmse))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_lr, best_l)\n",
        "print('best =', best_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72DoLu_0W2ec"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "L'andamento del validation è simile ai casi precedenti linear regression all'aumentare del numero di iterazioni e del learning rate.\n",
        "\n",
        "La differenza è che il valore selezionato di l (il peso della regolarizzazione) è maggiore in questo caso che nel caso di L2. Dato che la differenza principale tra questi due tipi di regolarizzazione è il fatto che L2 incoraggia alla riduzione di tutti i pesi ma non riesce ad azzerarne nessuno in modo che tutti i predittori verrano considerati anche se il loro impatto sarà mitigato. La regolarizzazione L1 invece riesce a portare alcuni pesi a 0, eliminando dall'equazione di fatto alcuni predittori non rilevanti.\n",
        "\n",
        "Un risultato del genere potrebbe suggerire la presenza nel dataset di predittori non molto utili al fine di stimare la colonna 'shares' e per i quali è benefico usare la regolarizzazione L1 con una certa forza, prima che questa diventi eccessiva e porti il modello ad essere poco rappresentativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-s4moJNbx9u",
        "outputId": "3074517e-55ae-4146-981f-45a05b43c42f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn RMSE: 6877.953923507106\n",
            "Test RMSE: 6877.22386280936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.971e+12, tolerance: 5.163e+08\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_W, test_bias = lassoRegression_train(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y],axis=0), best_e, best_l, best_lr)\n",
        "\n",
        "risultati = lassoRegression_predict(test_X, test_W, test_bias)\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lasso_model = Lasso(alpha=best_l, max_iter=best_e)\n",
        "lasso = Lasso()\n",
        "\n",
        "lasso_model.fit(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y],axis=0))\n",
        "\n",
        "sklearn_predictions = lasso_model.predict(test_X)\n",
        "sklearn_rmse_l = mean_squared_error(test_Y, sklearn_predictions, squared=False)\n",
        "print(\"Sklearn RMSE:\", sklearn_rmse_l)\n",
        "\n",
        "test_rmse_l = mean_squared_error(test_Y, risultati, squared=False)\n",
        "print(\"Test RMSE:\", test_rmse_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ZUPx6QQdzV"
      },
      "source": [
        "Anche in questo caso i valori sono praticamente uguali, con sklearn leggermente superiore in termini di errore commesso."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz0oATvf_QrK"
      },
      "source": [
        "#Linear Regression as Classification con regolarizzazione Lasso\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della classificazione con regressione lineare senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLAUXesX_hnp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "Y_binary = Y_binary.flatten()\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxcaV9ejXbxY"
      },
      "source": [
        "Anche qui lo stesso termine di regolarizzazione L1 è stato aggiunto al perceptron update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyJKc2xe_hns"
      },
      "outputs": [],
      "source": [
        "def lasso_classification_train(X, Y, epochs, lmbda, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.zeros(nFeatures)\n",
        "  bias = 0\n",
        "  for _ in range(epochs):\n",
        "    predictions = classification_predict(X, W, bias)\n",
        "    W += learning_rate * np.dot((Y-predictions), X) + lmbda * np.sign(W)\n",
        "    bias += learning_rate * np.sum((Y-predictions))\n",
        "  return W, bias\n",
        "\n",
        "def classification_predict(X, W, bias):\n",
        "  stime = np.dot(X, W) + bias\n",
        "  predictions = np.where(stime > 0, 1, 0)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW5Apcxv_hns",
        "outputId": "f0eda39a-3dc1-4e9e-ded9-e488d16bd475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.608728557013118\n",
            "iperparametri =  10 0.01 0.1 \n",
            "\tcurrent_acc = 0.6084762865792129\n",
            "\tbest_acc = 0.608728557013118\n",
            "iperparametri =  10 0.001 0.1 \n",
            "\tcurrent_acc = 0.613773965691221\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5840060544904138\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.1 0.2 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.01 0.2 \n",
            "\tcurrent_acc = 0.6102421796165489\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.001 0.2 \n",
            "\tcurrent_acc = 0.6125126135216953\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.0001 0.2 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6102421796165489\n",
            "\tbest_acc = 0.613773965691221\n",
            "iperparametri =  10 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6185671039354188\n",
            "iperparametri =  10 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6233602421796165\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.4 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.4 \n",
            "\tcurrent_acc = 0.6112512613521696\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.4 \n",
            "\tcurrent_acc = 0.6109989909182644\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6213420787083754\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.5 \n",
            "\tcurrent_acc = 0.6092330978809284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.5 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.5 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6188193743693239\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6079717457114027\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5900605449041373\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6178102926337034\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6084762865792129\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.613773965691221\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5953582240161454\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.8 \n",
            "\tcurrent_acc = 0.6097376387487387\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.8 \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.8 \n",
            "\tcurrent_acc = 0.572401614530777\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6178102926337034\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.1 0.9 \n",
            "\tcurrent_acc = 0.6084762865792129\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.01 0.9 \n",
            "\tcurrent_acc = 0.6127648839556004\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.001 0.9 \n",
            "\tcurrent_acc = 0.5928355196770938\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  10 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.1 \n",
            "\tcurrent_acc = 0.5262361251261353\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.1 \n",
            "\tcurrent_acc = 0.5259838546922301\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.1 \n",
            "\tcurrent_acc = 0.5857719475277497\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5908173562058526\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.2 \n",
            "\tcurrent_acc = 0.5257315842583249\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.2 \n",
            "\tcurrent_acc = 0.5232088799192735\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.2 \n",
            "\tcurrent_acc = 0.5194248234106963\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.2 \n",
            "\tcurrent_acc = 0.5951059535822402\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.5262361251261353\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.5254793138244198\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.5358224016145308\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6218466195761857\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.4 \n",
            "\tcurrent_acc = 0.5249747729566094\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.4 \n",
            "\tcurrent_acc = 0.5976286579212916\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.4 \n",
            "\tcurrent_acc = 0.5605449041372351\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6226034308779012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.5 \n",
            "\tcurrent_acc = 0.5219475277497477\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.5 \n",
            "\tcurrent_acc = 0.5875378405650857\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.5 \n",
            "\tcurrent_acc = 0.558526740665994\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5244702320887992\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5877901109989909\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5726538849646822\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5239656912209889\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5880423814328961\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.8 \n",
            "\tcurrent_acc = 0.5239656912209889\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.8 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.8 \n",
            "\tcurrent_acc = 0.5872855701311807\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.1 0.9 \n",
            "\tcurrent_acc = 0.5242179616548941\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.01 0.9 \n",
            "\tcurrent_acc = 0.5191725529767911\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.001 0.9 \n",
            "\tcurrent_acc = 0.5905650857719476\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  100 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.1 \n",
            "\tcurrent_acc = 0.5257315842583249\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.1 \n",
            "\tcurrent_acc = 0.527497477295661\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.1 \n",
            "\tcurrent_acc = 0.5751765893037336\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.1 \n",
            "\tcurrent_acc = 0.5908173562058526\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.2 \n",
            "\tcurrent_acc = 0.5277497477295661\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.2 \n",
            "\tcurrent_acc = 0.5249747729566094\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.2 \n",
            "\tcurrent_acc = 0.5595358224016145\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.2 \n",
            "\tcurrent_acc = 0.5933400605449042\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.30000000000000004 \n",
            "\tcurrent_acc = 0.5272452068617558\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.30000000000000004 \n",
            "\tcurrent_acc = 0.529263370332997\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.5592835519677094\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.30000000000000004 \n",
            "\tcurrent_acc = 0.6213420787083754\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.4 \n",
            "\tcurrent_acc = 0.527497477295661\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.4 \n",
            "\tcurrent_acc = 0.5981331987891019\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.4 \n",
            "\tcurrent_acc = 0.5749243188698284\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.4 \n",
            "\tcurrent_acc = 0.6226034308779012\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.5 \n",
            "\tcurrent_acc = 0.5272452068617558\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.5 \n",
            "\tcurrent_acc = 0.5782038345105953\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.5 \n",
            "\tcurrent_acc = 0.5875378405650857\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.5 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5272452068617558\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5857719475277497\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.5893037336024218\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.6000000000000001 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5282542885973764\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5832492431886983\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.5850151362260343\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.7000000000000001 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.8 \n",
            "\tcurrent_acc = 0.5287588294651867\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.8 \n",
            "\tcurrent_acc = 0.5915741675075681\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.8 \n",
            "\tcurrent_acc = 0.5872855701311807\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.8 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.1 0.9 \n",
            "\tcurrent_acc = 0.5282542885973764\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.01 0.9 \n",
            "\tcurrent_acc = 0.5295156407669022\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.001 0.9 \n",
            "\tcurrent_acc = 0.593087790110999\n",
            "\tbest_acc = 0.6233602421796165\n",
            "iperparametri =  500 0.0001 0.9 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6233602421796165\n",
            "======================================================================================================\n",
            "best_iperparametri =  10 0.0001 0.30000000000000004\n",
            "best = 0.6233602421796165\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochses = [10, 100, 500]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "lmbdas = [i*10**-1 for i in range(1, 10)]\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for l in lmbdas:\n",
        "    for lr in rates:\n",
        "\n",
        "      W, bias = lasso_classification_train(train_X, train_Y, e, l, lr)\n",
        "      predictions = classification_predict(validation_X, W, bias)\n",
        "\n",
        "      current_acc = accuracy_score(validation_Y, predictions)\n",
        "\n",
        "      if( current_acc >= best_acc):\n",
        "        best_acc = current_acc\n",
        "        best_e, best_lr, best_l = e, lr, l\n",
        "      print('iperparametri = ', e, lr, l, '\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_lr, best_l)\n",
        "print('best =', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rYvGpAoQuvY"
      },
      "source": [
        "Valgono considerazioni simili a quelle fatte nel caso della classificazione con regressione e regolarizzazione L2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8rZbMym_hnt",
        "outputId": "daf010df-11a9-4dce-8912-dbf256cc1a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_acc = 0.6269861286254729\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias = lasso_classification_train(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0), best_e, best_l, best_lr)\n",
        "test_predictions = classification_predict(test_X, test_W, test_bias)\n",
        "test_acc_l = accuracy_score(test_Y, test_predictions)\n",
        "\n",
        "print('test_acc = ' + str(test_acc_l))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "escQ3RAYGY90",
        "outputId": "9a71dcf0-bb1d-4270-e82a-2805579e4410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn acc: 0.6453972257250946\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "classifier = SGDClassifier( penalty='l1', eta0=0.0001, learning_rate='constant', max_iter=100)\n",
        "\n",
        "classifier.fit(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0))\n",
        "\n",
        "sklearn_predictions = classifier.predict(test_X)\n",
        "\n",
        "sklearn_acc_l = accuracy_score(test_Y, sklearn_predictions)\n",
        "print(\"Sklearn acc:\", sklearn_acc_l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hGfhTNe8ils"
      },
      "source": [
        "# Linear Regression con regolarizzazione Elastic Net\n",
        "\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della regressione senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTf311Bk8v5V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLMW623y89IB"
      },
      "outputs": [],
      "source": [
        "nFeatures = len(feature_names)\n",
        "W = np.random.random_sample(nFeatures)\n",
        "bias = np.random.random_sample()\n",
        "predictions = np.dot(test_X,W) + bias\n",
        "\n",
        "\n",
        "def costo(true_Y, pred_Y, alpha, lmbda):\n",
        "  n = true_Y.shape[0]\n",
        "  loss = (1/n) * np.sum(np.square(true_Y.values.flatten() - pred_Y.flatten())) + alpha(lmbda*np.sum(np.abs(W)) + (1-lmbda)*np.sum(np.square(W)))\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaoMvyRjca3K"
      },
      "source": [
        "il termine di regolarizzazione in questo caso è la somma dei termini L1 ed L2, moltiplicati per una frazione che indica quando, nell'elastic net sia più rilevante il termine L1 o quello L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9CZbehP9hdz"
      },
      "outputs": [],
      "source": [
        "def elastic_calcola_gradient(X, Y, predictions, alpha, lmbda):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  for i in range(nFeatures):\n",
        "      gradiente[i] = -(2/nCampioni) * (np.dot(X.values[:, i], (Y.values.flatten() - predictions)) + alpha * (lmbda * np.sign(W[i]) + (1 - lmbda) * 2 * W[i]))\n",
        "  return gradiente\n",
        "\n",
        "\n",
        "def calcola_gradiente_bias(X, Y, predictions, lmbda):\n",
        "\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  gradiente = np.zeros(nFeatures)\n",
        "\n",
        "  gradient_bias = -(2/nCampioni) * np.sum(Y.values.flatten() - predictions)\n",
        "  return gradient_bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSSU6VyQB0K5"
      },
      "outputs": [],
      "source": [
        "def elasticRegression_train(X, Y, epochs, lmbda, alpha, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.random.random_sample(nFeatures)\n",
        "  bias = np.random.random_sample()\n",
        "  for _ in range(epochs):\n",
        "    predictions = np.dot(X,W) + bias\n",
        "    gradient = elastic_calcola_gradient(X, Y, predictions, alpha, lmbda)\n",
        "    gradient_bias = calcola_gradiente_bias(X, Y, predictions, lmbda)\n",
        "    W -= learning_rate * gradient\n",
        "    bias -= learning_rate * gradient_bias\n",
        "  return W, bias\n",
        "\n",
        "def elasticRegression_predict(X, W, bias):\n",
        "  return np.dot(X, W) + bias;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3IrRKjsrG4a",
        "outputId": "9054c83d-4f7e-40ad-e6ea-be08eb7e5a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 0.1 \n",
            "\tcurrent_rmse = 7757.678115141096\n",
            "\tbest_rmse = 7757.678115141096\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 7757.665125762665\n",
            "\tbest_rmse = 7757.665125762665\n",
            "iperparametri =  10 0.1 0.5 0.1 \n",
            "\tcurrent_rmse = 7757.673240665229\n",
            "\tbest_rmse = 7757.665125762665\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 7757.696970208083\n",
            "\tbest_rmse = 7757.665125762665\n",
            "iperparametri =  10 0.1 0.9 0.1 \n",
            "\tcurrent_rmse = 7757.661293809707\n",
            "\tbest_rmse = 7757.661293809707\n",
            "iperparametri =  10 0.1 0.1 0.3 \n",
            "\tcurrent_rmse = 7757.6551205263695\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 7757.6690566650095\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.5 0.3 \n",
            "\tcurrent_rmse = 7757.678042341726\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 7757.679603346257\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.9 0.3 \n",
            "\tcurrent_rmse = 7757.677217120307\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.1 0.5 \n",
            "\tcurrent_rmse = 7757.6711375743425\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 7757.687446358693\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.5 0.5 \n",
            "\tcurrent_rmse = 7757.684272150666\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 7757.695098251895\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.9 0.5 \n",
            "\tcurrent_rmse = 7757.663058721475\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.1 0.7 \n",
            "\tcurrent_rmse = 7757.676266684998\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 7757.6730408535905\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.5 0.7 \n",
            "\tcurrent_rmse = 7757.676152015931\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 7757.664490280058\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.1 0.9 0.7 \n",
            "\tcurrent_rmse = 7757.677559132258\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.1 0.1 \n",
            "\tcurrent_rmse = 8568.520031392856\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 8568.214042266105\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.5 0.1 \n",
            "\tcurrent_rmse = 8568.19609732185\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 8568.326337870916\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.9 0.1 \n",
            "\tcurrent_rmse = 8568.172271305604\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.1 0.3 \n",
            "\tcurrent_rmse = 8568.58473744609\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 8568.43166880994\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.5 0.3 \n",
            "\tcurrent_rmse = 8568.450580705581\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 8568.548285247069\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.9 0.3 \n",
            "\tcurrent_rmse = 8568.829947522658\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.1 0.5 \n",
            "\tcurrent_rmse = 8568.488189963091\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 8568.468790558836\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.5 0.5 \n",
            "\tcurrent_rmse = 8568.355390342496\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 8568.485871817491\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.9 0.5 \n",
            "\tcurrent_rmse = 8568.284610888148\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.1 0.7 \n",
            "\tcurrent_rmse = 8568.41154628832\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 8568.370487103844\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.5 0.7 \n",
            "\tcurrent_rmse = 8568.720549683603\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 8568.273529131015\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  10 0.001 0.9 0.7 \n",
            "\tcurrent_rmse = 8568.366534049723\n",
            "\tbest_rmse = 7757.6551205263695\n",
            "iperparametri =  250 0.1 0.1 0.1 \n",
            "\tcurrent_rmse = 7730.416963150058\n",
            "\tbest_rmse = 7730.416963150058\n",
            "iperparametri =  250 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 7730.415469105628\n",
            "\tbest_rmse = 7730.415469105628\n",
            "iperparametri =  250 0.1 0.5 0.1 \n",
            "\tcurrent_rmse = 7730.412230973357\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 7730.412681704479\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.9 0.1 \n",
            "\tcurrent_rmse = 7730.415006542146\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.1 0.3 \n",
            "\tcurrent_rmse = 7730.413870103543\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 7730.413355516577\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.5 0.3 \n",
            "\tcurrent_rmse = 7730.4145962164075\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 7730.413093806074\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.9 0.3 \n",
            "\tcurrent_rmse = 7730.4156327591345\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.1 0.5 \n",
            "\tcurrent_rmse = 7730.41389451023\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 7730.413249243329\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.5 0.5 \n",
            "\tcurrent_rmse = 7730.4130124259655\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 7730.414624762003\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.9 0.5 \n",
            "\tcurrent_rmse = 7730.413207725483\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.1 0.7 \n",
            "\tcurrent_rmse = 7730.412254585076\n",
            "\tbest_rmse = 7730.412230973357\n",
            "iperparametri =  250 0.1 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 7730.409615128445\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.1 0.5 0.7 \n",
            "\tcurrent_rmse = 7730.412497832534\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.1 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 7730.415657511856\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.1 0.9 0.7 \n",
            "\tcurrent_rmse = 7730.412189680602\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.1 0.1 \n",
            "\tcurrent_rmse = 8032.285576351731\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 8032.242315021117\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.5 0.1 \n",
            "\tcurrent_rmse = 8032.2144725329945\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 8032.188950431254\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.9 0.1 \n",
            "\tcurrent_rmse = 8032.124506924364\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.1 0.3 \n",
            "\tcurrent_rmse = 8032.269661046946\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 8032.131467818419\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.5 0.3 \n",
            "\tcurrent_rmse = 8032.138503014842\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 8032.176253203344\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.9 0.3 \n",
            "\tcurrent_rmse = 8032.107605912659\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.1 0.5 \n",
            "\tcurrent_rmse = 8032.268729084729\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 8032.177811268265\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.5 0.5 \n",
            "\tcurrent_rmse = 8032.216636927005\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 8032.089248078673\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.9 0.5 \n",
            "\tcurrent_rmse = 8032.164376411309\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.1 0.7 \n",
            "\tcurrent_rmse = 8032.254989047971\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 8032.180984752362\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.5 0.7 \n",
            "\tcurrent_rmse = 8032.145168876098\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 8032.212353385126\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  250 0.001 0.9 0.7 \n",
            "\tcurrent_rmse = 8032.264954890809\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.1 0.1 \n",
            "\tcurrent_rmse = 7731.133678049346\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 7731.133909855033\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.5 0.1 \n",
            "\tcurrent_rmse = 7731.133878999605\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 7731.134483455735\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.9 0.1 \n",
            "\tcurrent_rmse = 7731.134116141206\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.1 0.3 \n",
            "\tcurrent_rmse = 7731.134929242102\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 7731.135483737629\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.5 0.3 \n",
            "\tcurrent_rmse = 7731.134678360073\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 7731.134294924702\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.9 0.3 \n",
            "\tcurrent_rmse = 7731.133277578346\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.1 0.5 \n",
            "\tcurrent_rmse = 7731.135978729719\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 7731.135622589435\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.5 0.5 \n",
            "\tcurrent_rmse = 7731.135406338525\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 7731.133999084185\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.9 0.5 \n",
            "\tcurrent_rmse = 7731.134232071517\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.1 0.7 \n",
            "\tcurrent_rmse = 7731.136783774966\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 7731.136716777882\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.5 0.7 \n",
            "\tcurrent_rmse = 7731.135384757565\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 7731.135006194077\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.1 0.9 0.7 \n",
            "\tcurrent_rmse = 7731.13457341268\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.1 0.1 \n",
            "\tcurrent_rmse = 7851.543757614887\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.30000000000000004 0.1 \n",
            "\tcurrent_rmse = 7851.547054884194\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.5 0.1 \n",
            "\tcurrent_rmse = 7851.5575712379205\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.7000000000000001 0.1 \n",
            "\tcurrent_rmse = 7851.568901873343\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.9 0.1 \n",
            "\tcurrent_rmse = 7851.519505425867\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.1 0.3 \n",
            "\tcurrent_rmse = 7851.537954629302\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.30000000000000004 0.3 \n",
            "\tcurrent_rmse = 7851.513576861942\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.5 0.3 \n",
            "\tcurrent_rmse = 7851.59879820161\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.7000000000000001 0.3 \n",
            "\tcurrent_rmse = 7851.4992901177175\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.9 0.3 \n",
            "\tcurrent_rmse = 7851.530766283721\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.1 0.5 \n",
            "\tcurrent_rmse = 7851.515762093966\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.30000000000000004 0.5 \n",
            "\tcurrent_rmse = 7851.553875629523\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.5 0.5 \n",
            "\tcurrent_rmse = 7851.555169713809\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.7000000000000001 0.5 \n",
            "\tcurrent_rmse = 7851.541662454827\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.9 0.5 \n",
            "\tcurrent_rmse = 7851.523371076849\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.1 0.7 \n",
            "\tcurrent_rmse = 7851.519122234068\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.30000000000000004 0.7 \n",
            "\tcurrent_rmse = 7851.48308932517\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.5 0.7 \n",
            "\tcurrent_rmse = 7851.567970950372\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.7000000000000001 0.7 \n",
            "\tcurrent_rmse = 7851.552919253796\n",
            "\tbest_rmse = 7730.409615128445\n",
            "iperparametri =  500 0.001 0.9 0.7 \n",
            "\tcurrent_rmse = 7851.601257679423\n",
            "\tbest_rmse = 7730.409615128445\n",
            "======================================================================================================\n",
            "best_iperparametri =  250 0.1 0.30000000000000004 0.7\n",
            "best = 7730.409615128445\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "epochses = [10, 250, 500]\n",
        "rates = [10**i for i in range(-1, -5, -2)]\n",
        "lmbdas = [i*10**-1 for i in range(1, 10, 2)]\n",
        "alphas = [0.1, 0.3, 0.5, 0.7]\n",
        "best_rmse = float('inf')\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "    for a in alphas:\n",
        "      for l in lmbdas :\n",
        "        W, bias = elasticRegression_train(train_X, train_Y, e, l, a, lr)\n",
        "        predictions = elasticRegression_predict(validation_X, W, bias)\n",
        "        current_rmse = mean_squared_error(validation_Y, predictions, squared=False)\n",
        "\n",
        "        if( current_rmse < best_rmse):\n",
        "          best_rmse = current_rmse\n",
        "          best_e, best_lr, best_l, best_a = e, lr, l, a\n",
        "        print('iperparametri = ', e, lr, l, a, '\\n\\tcurrent_rmse = '+str(current_rmse)+ '\\n\\tbest_rmse = '+str(best_rmse))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_lr, best_l, best_a)\n",
        "print('best =', best_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1moiprNRnS6"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "Rispetto ai modelli precedenti, la considerazione che si può aggiungere in questo caso è che il valore di l scelto (che in questo caso indica quanto prevale la regolarizzazione L1 rispetto a quella L2 nell'elastic net) è il 70%, e quindi prevale rispetto alla proporzione di L2 del 30%.\n",
        "\n",
        "In effetti in precedenza avevamo già visto come un peso elevato della regolarizzazione L2, portasse ad un declino della capacità del modello di generalizzare, mentre la regolarizzazione L1 continuava a migliorare le prestazioni fino al peso di 0.7. Di fatto la situazione qui è simile. Si ha il termine di regolarizzazione L1 moltiplicato ancora una volta per 0.7. Quello della regolarizzazione L2 per 0.3, un valore più alto di quello che sapevamo essere ottimale per la regressione Ridge. Questo ci fa supporre che riavvicinare il peso di L2 a quello ottimale (e quindi diminuirlo, in favore di un aumento di L1), avrebbe peggiorato le prestazioni, perchè l'aumento di L1 avrebbe ipersemplificato il modello più di quanto un decremento di L2 lo avrebbe reso rappresentativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OPjaie9q-Zy",
        "outputId": "8fac41bd-91c0-4199-e511-dbd5a22525e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn RMSE 6885.14169696292\n",
            "My RMSE: 6877.224969408623\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias = elasticRegression_train(pd.concat([train_X, validation_X], axis=0), pd.concat([train_Y, validation_Y],axis=0), best_e, best_l, best_a, best_lr)\n",
        "risultati = elasticRegression_predict(test_X, test_W, test_bias)\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "elastic_net = ElasticNet(alpha=best_a, l1_ratio=best_l, max_iter=best_e)\n",
        "\n",
        "elastic_net.fit(train_X, train_Y)\n",
        "\n",
        "predictions = elastic_net.predict(test_X)\n",
        "\n",
        "sklearn_rmse_e = mean_squared_error(test_Y, predictions, squared=False)\n",
        "print('sklearn RMSE', sklearn_rmse_e)\n",
        "\n",
        "test_rmse_e = mean_squared_error(test_Y, risultati, squared=False)\n",
        "print(\"My RMSE:\", test_rmse_e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEchbw_VDQIX"
      },
      "source": [
        "# Linear Regression as Classification con Elastic Net\n",
        "\n",
        "Il preprocessing dei dati e i suoi motivi sono identici a quelli della classificazione con regressione lineare senza regolarizzazione"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmHvOs9HAU2T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "Y_binary = Y_binary.flatten()\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er8heHOWAU2V"
      },
      "outputs": [],
      "source": [
        "def elastic_classification_train(X, Y, epochs, alpha, lmbda, learning_rate):\n",
        "  nFeatures = X.shape[1]\n",
        "  W = np.zeros(nFeatures)\n",
        "  bias = 0\n",
        "  for _ in range(epochs):\n",
        "    predictions = classification_predict(X, W, bias)\n",
        "    W += learning_rate * np.dot((Y-predictions), X) + lmbda * np.sign(W) + (1-lmbda) * W\n",
        "    bias += learning_rate * np.sum((Y-predictions))\n",
        "  return W, bias\n",
        "\n",
        "def classification_predict(X, W, bias):\n",
        "  stime = np.dot(X, W) + bias\n",
        "  predictions = np.where(stime > 0, 1, 0)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2pmeyLdAU2W",
        "outputId": "b6abc066-e80e-41dd-f32b-82887abaab10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 0.1 0.1 \n",
            "\tcurrent_acc = 0.612008072653885\n",
            "\tbest_acc = 0.612008072653885\n",
            "iperparametri =  10 0.1 0.1 0.01 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.6130171543895055\n",
            "iperparametri =  10 0.1 0.1 0.001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6185671039354188\n",
            "iperparametri =  10 0.1 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6203329969727548\n",
            "iperparametri =  10 0.1 0.2 0.1 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6203329969727548\n",
            "iperparametri =  10 0.1 0.2 0.01 \n",
            "\tcurrent_acc = 0.6089808274470232\n",
            "\tbest_acc = 0.6203329969727548\n",
            "iperparametri =  10 0.1 0.2 0.001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6203329969727548\n",
            "iperparametri =  10 0.1 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6238647830474269\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.6019172552976791\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.6014127144298688\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.4 0.1 \n",
            "\tcurrent_acc = 0.6099899091826438\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.4 0.01 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.4 0.001 \n",
            "\tcurrent_acc = 0.6104944500504541\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.5 0.1 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.5 0.01 \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.5 0.001 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6193239152371343\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6218466195761857\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6109989909182644\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6208375378405651\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6238647830474269\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6296670030272452\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.605196770938446\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.8 0.1 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.8 0.01 \n",
            "\tcurrent_acc = 0.624117053481332\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.8 0.001 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.9 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.9 0.01 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.9 0.001 \n",
            "\tcurrent_acc = 0.606962663975782\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.1 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.1 0.1 \n",
            "\tcurrent_acc = 0.612008072653885\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.1 0.01 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.1 0.001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.2 0.1 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.2 0.01 \n",
            "\tcurrent_acc = 0.6089808274470232\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.2 0.001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6238647830474269\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.6019172552976791\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.6014127144298688\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.4 0.1 \n",
            "\tcurrent_acc = 0.6099899091826438\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.4 0.01 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.4 0.001 \n",
            "\tcurrent_acc = 0.6104944500504541\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.5 0.1 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.5 0.01 \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.5 0.001 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6193239152371343\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6218466195761857\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6109989909182644\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6208375378405651\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6296670030272452\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.605196770938446\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.8 0.1 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.8 0.01 \n",
            "\tcurrent_acc = 0.624117053481332\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.8 0.001 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.9 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.9 0.01 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.9 0.001 \n",
            "\tcurrent_acc = 0.606962663975782\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.3 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.1 0.1 \n",
            "\tcurrent_acc = 0.612008072653885\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.1 0.01 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.1 0.001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.2 0.1 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.2 0.01 \n",
            "\tcurrent_acc = 0.6089808274470232\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.2 0.001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6238647830474269\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.6019172552976791\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.6014127144298688\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.4 0.1 \n",
            "\tcurrent_acc = 0.6099899091826438\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.4 0.01 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.4 0.001 \n",
            "\tcurrent_acc = 0.6104944500504541\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.5 0.1 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.5 0.01 \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.5 0.001 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6193239152371343\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6218466195761857\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6109989909182644\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6208375378405651\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6296670030272452\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.605196770938446\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.8 0.1 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.8 0.01 \n",
            "\tcurrent_acc = 0.624117053481332\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.8 0.001 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.9 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.9 0.01 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.9 0.001 \n",
            "\tcurrent_acc = 0.606962663975782\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.5 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.1 0.1 \n",
            "\tcurrent_acc = 0.612008072653885\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.1 0.01 \n",
            "\tcurrent_acc = 0.6130171543895055\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.1 0.001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.2 0.1 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.2 0.01 \n",
            "\tcurrent_acc = 0.6089808274470232\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.2 0.001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6238647830474269\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.6019172552976791\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.6014127144298688\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.4 0.1 \n",
            "\tcurrent_acc = 0.6099899091826438\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.4 0.01 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.4 0.001 \n",
            "\tcurrent_acc = 0.6104944500504541\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.5 0.1 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.5 0.01 \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.5 0.001 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6193239152371343\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6218466195761857\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6109989909182644\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.608728557013118\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6208375378405651\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6215943491422805\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6296670030272452\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.605196770938446\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.8 0.1 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.8 0.01 \n",
            "\tcurrent_acc = 0.624117053481332\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.8 0.001 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6175580221997982\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.9 0.1 \n",
            "\tcurrent_acc = 0.6117558022199798\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.9 0.01 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.9 0.001 \n",
            "\tcurrent_acc = 0.606962663975782\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  10 0.7 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6168012108980827\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.9 0.01 \n",
            "\tcurrent_acc = 0.5961150353178607\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.9 0.001 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.1 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.9 0.01 \n",
            "\tcurrent_acc = 0.5961150353178607\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.9 0.001 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.3 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.9 0.01 \n",
            "\tcurrent_acc = 0.5961150353178607\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.9 0.001 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.5 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.9 0.01 \n",
            "\tcurrent_acc = 0.5961150353178607\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.9 0.001 \n",
            "\tcurrent_acc = 0.5887991927346115\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  100 0.7 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.9 0.01 \n",
            "\tcurrent_acc = 0.5953582240161454\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.9 0.001 \n",
            "\tcurrent_acc = 0.5893037336024218\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.1 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.9 0.01 \n",
            "\tcurrent_acc = 0.5953582240161454\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.9 0.001 \n",
            "\tcurrent_acc = 0.5893037336024218\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.3 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.9 0.01 \n",
            "\tcurrent_acc = 0.5953582240161454\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.9 0.001 \n",
            "\tcurrent_acc = 0.5893037336024218\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.5 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.1 0.1 \n",
            "\tcurrent_acc = 0.6122603430877901\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.1 0.01 \n",
            "\tcurrent_acc = 0.6107467204843592\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.1 0.001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.1 0.0001 \n",
            "\tcurrent_acc = 0.6203329969727548\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.2 0.1 \n",
            "\tcurrent_acc = 0.6049445005045408\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.2 0.01 \n",
            "\tcurrent_acc = 0.6072149344096872\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.2 0.001 \n",
            "\tcurrent_acc = 0.6142785065590313\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.2 0.0001 \n",
            "\tcurrent_acc = 0.6236125126135217\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.30000000000000004 0.1 \n",
            "\tcurrent_acc = 0.591321897073663\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.30000000000000004 0.01 \n",
            "\tcurrent_acc = 0.5910696266397578\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.30000000000000004 0.001 \n",
            "\tcurrent_acc = 0.6180625630676084\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.30000000000000004 0.0001 \n",
            "\tcurrent_acc = 0.6195761856710393\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.4 0.1 \n",
            "\tcurrent_acc = 0.5958627648839556\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.4 0.01 \n",
            "\tcurrent_acc = 0.5943491422805247\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.4 0.001 \n",
            "\tcurrent_acc = 0.6034308779011099\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.4 0.0001 \n",
            "\tcurrent_acc = 0.6210898082744702\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.5 0.1 \n",
            "\tcurrent_acc = 0.6082240161453077\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.5 0.01 \n",
            "\tcurrent_acc = 0.606710393541877\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.5 0.001 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.5 0.0001 \n",
            "\tcurrent_acc = 0.6190716448032291\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.6000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6162966700302724\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.6000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6094853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.6000000000000001 0.001 \n",
            "\tcurrent_acc = 0.5983854692230071\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.6000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6185671039354188\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.7000000000000001 0.1 \n",
            "\tcurrent_acc = 0.6029263370332997\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.7000000000000001 0.01 \n",
            "\tcurrent_acc = 0.6064581231079718\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.7000000000000001 0.001 \n",
            "\tcurrent_acc = 0.594853683148335\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.7000000000000001 0.0001 \n",
            "\tcurrent_acc = 0.6170534813319879\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.8 0.1 \n",
            "\tcurrent_acc = 0.5963673057517659\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.8 0.01 \n",
            "\tcurrent_acc = 0.5993945509586277\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.8 0.001 \n",
            "\tcurrent_acc = 0.5898082744702321\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.8 0.0001 \n",
            "\tcurrent_acc = 0.6165489404641776\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.9 0.1 \n",
            "\tcurrent_acc = 0.5882946518668012\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.9 0.01 \n",
            "\tcurrent_acc = 0.5953582240161454\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.9 0.001 \n",
            "\tcurrent_acc = 0.5893037336024218\n",
            "\tbest_acc = 0.6296670030272452\n",
            "iperparametri =  500 0.7 0.9 0.0001 \n",
            "\tcurrent_acc = 0.6160443995963673\n",
            "\tbest_acc = 0.6296670030272452\n",
            "======================================================================================================\n",
            "best_iperparametri =  10 0.7 0.01 0.7000000000000001\n",
            "best = 0.6296670030272452\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochses = [10, 100, 500]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "lmbdas = [i*10**-1 for i in range(1, 10)]\n",
        "alphas = [0.1, 0.3, 0.5, 0.7]\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for a in alphas:\n",
        "    for l in lmbdas:\n",
        "      for lr in rates:\n",
        "\n",
        "        W, bias = elastic_classification_train(train_X, train_Y, e, a, l, lr)\n",
        "        predictions = classification_predict(validation_X, W, bias)\n",
        "\n",
        "        current_acc = accuracy_score(validation_Y, predictions)\n",
        "\n",
        "        if( current_acc >= best_acc):\n",
        "          best_acc = current_acc\n",
        "          best_e, best_lr, best_l, best_a = e, lr, l, a\n",
        "        print('iperparametri = ', e, a, l, lr, '\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_a, best_lr, best_l)\n",
        "print('best =', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3eral_lAU2Y",
        "outputId": "443fef15-8bef-4c6f-ba7b-f261cf729a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_acc = 0.6421185372005044\n"
          ]
        }
      ],
      "source": [
        "test_W, test_bias = elastic_classification_train(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0), best_e, best_a, best_l, best_lr)\n",
        "test_predictions = classification_predict(test_X, test_W, test_bias)\n",
        "test_acc_e = accuracy_score(test_Y, test_predictions)\n",
        "\n",
        "print('test_acc = ' + str(test_acc_e))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7DfWIA5GdI2",
        "outputId": "1d2b07dc-3a13-4380-e7b7-fb3ba1299684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn acc: 0.6305170239596469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "classifier = SGDClassifier(penalty=\"elasticnet\",  learning_rate='constant', max_iter=best_e, eta0=best_lr, l1_ratio=best_l)\n",
        "\n",
        "pipeline = Pipeline([('classifier', classifier)])\n",
        "\n",
        "pipeline.fit(pd.concat([train_X, validation_X], axis=0), np.concatenate((train_Y, validation_Y), axis=0))\n",
        "\n",
        "sklearn_predictions = pipeline.predict(test_X)\n",
        "\n",
        "sklearn_acc_e = accuracy_score(test_Y, sklearn_predictions)\n",
        "print(\"Sklearn acc:\", sklearn_acc_e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtxt6xAnWc6J"
      },
      "source": [
        "Anche qui il risultato ottenuto da sklearn è leggermente inferiore a quello proposto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hnuNPqZckgr"
      },
      "source": [
        "# KNN classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Api1U17hSk8i"
      },
      "source": [
        "In questo caso non è stato necessario dividere la colonna target dalle features predittive. E stato come al solito discretizzato il target, e normalizzate le features. Questo passaggio è particolarmente utile in quanto evita che il calcolo delle distanze sia distorto da una feature la cui differenza è elevata ma perchè in generale i suoi valori sono elevati e non riflette una reale distanza tra i campioni."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVy_48K6cm3K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "KNN_samples = df.copy()\n",
        "median = KNN_samples[target_name].median()\n",
        "\n",
        "KNN_samples[target_name] = np.where( KNN_samples[target_name] >= median, 1, 0)    #discretizzazione\n",
        "\n",
        "KNN_samples[feature_names] = (KNN_samples[feature_names] - KNN_samples[feature_names].mean(axis=0)) / KNN_samples[feature_names].std(axis=0)  #normalizzazione\n",
        "\n",
        "KNN_train, KNN_test = train_test_split(KNN_samples, test_size=0.2 , random_state=25)\n",
        "\n",
        "KNN_validation, KNN_test = train_test_split(KNN_test, test_size=0.5, random_state=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHWwUva6T8PM"
      },
      "source": [
        "L'algoritmo non ha bisogno di una fase di training in quanto il training è costituito dal Training Set in se, che agisce come una tabella da poter consultare. Ogni campione viene confrontato con quelli conosciuti presenti nel training set e si cercano i k più vicini, nell'ipotesi che il campione in questione sia simile anche nella colonna target.\n",
        "\n",
        "Per efficienza si è usata la funzione cdist di scipy che oltre ad essere molto veloce consente anche di scegliere il tipo di distanza da utilizzare che è stato considerato come un iperparametro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6HY_Ol2fQqZ"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def KNN_clas_predict(X, distance, k):\n",
        "  predictions = []\n",
        "\n",
        "  distanze = cdist(X.values, KNN_train[feature_names].values, distance)\n",
        "  target_vicini = KNN_train[target_name].values[ np.argsort(distanze, axis = 1)[:,:k]]\n",
        "  predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), 1, target_vicini)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVVW7AMQUhRo"
      },
      "source": [
        "In fase di Validation si testano diversi valori di K e diversi tipi di distanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jISyARkHJbt",
        "outputId": "4c2f2e16-5c23-47f3-8f7a-f4ec76614255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  9 cityblock \n",
            "\tcurrent_acc = 0.6135216952573158\n",
            "\tbest_acc = 0.6135216952573158\n",
            "iperparametri =  27 cityblock \n",
            "\tcurrent_acc = 0.6299192734611504\n",
            "\tbest_acc = 0.6299192734611504\n",
            "iperparametri =  45 cityblock \n",
            "\tcurrent_acc = 0.6374873864783047\n",
            "\tbest_acc = 0.6374873864783047\n",
            "iperparametri =  63 cityblock \n",
            "\tcurrent_acc = 0.641271442986882\n",
            "\tbest_acc = 0.641271442986882\n",
            "iperparametri =  81 cityblock \n",
            "\tcurrent_acc = 0.6442986881937437\n",
            "\tbest_acc = 0.6442986881937437\n",
            "iperparametri =  99 cityblock \n",
            "\tcurrent_acc = 0.6453077699293642\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  9 euclidean \n",
            "\tcurrent_acc = 0.6243693239152371\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  27 euclidean \n",
            "\tcurrent_acc = 0.6304238143289607\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  45 euclidean \n",
            "\tcurrent_acc = 0.6390010090817356\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  63 euclidean \n",
            "\tcurrent_acc = 0.6372351160443996\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  81 euclidean \n",
            "\tcurrent_acc = 0.6342078708375378\n",
            "\tbest_acc = 0.6453077699293642\n",
            "iperparametri =  99 euclidean \n",
            "\tcurrent_acc = 0.6326942482341069\n",
            "\tbest_acc = 0.6453077699293642\n",
            "======================================================================================================\n",
            "best_iperparametri =  99 cityblock\n",
            "best = 0.6453077699293642\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "ks = [i for i in range(0,100,9)]\n",
        "distances = ['cityblock', 'euclidean']\n",
        "best_acc = 0\n",
        "\n",
        "for d in distances:\n",
        "  for k in ks:\n",
        "    if(k%2 == 0): continue\n",
        "    predictions = KNN_clas_predict(KNN_validation[feature_names], d, k)\n",
        "\n",
        "    current_acc = accuracy_score(KNN_validation['shares'], predictions)\n",
        "\n",
        "    if( current_acc >= best_acc):\n",
        "      best_acc = current_acc\n",
        "      best_d, best_k = d, k\n",
        "    print('iperparametri = ', k, d, '\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_k, best_d)\n",
        "print('best =', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK2kUJrwke_-"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "L'algoritmo KNN sembra funzionare meglio sul dataset che abbiamo con un K alto. Questo si verifica in genere quando le classi sono separate più o meno linearmente e non hanno intromissioni spaziali una nell'altra. Possiamo immaginarlo meglio in due dimensioni: se le classi avessero delle insenature, delle \"penisole\" o delle \"isole\", i punti di queste zone avrebbero molti punti vicini che però appartengolo all'altra classe. Se si usa un k elevato, il rischio è quello di supporre che sia dell'altra classe, perchè un k alto porta a ignorare i pattern e le caratteristiche locali.\n",
        "Se nel nostro caso un K molto elevato funziona meglio di qualunque altro K, vuol dire che le due classi, nel nostro dataset, sono molto ben separate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqfcQvzT95Co",
        "outputId": "694b0fef-c620-41c4-884c-0d436b515a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6489281210592686\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "predictions = KNN_clas_predict(KNN_test[feature_names], best_d, best_k)\n",
        "test_acc_k = metrics.accuracy_score(KNN_test[target_name], predictions)\n",
        "print(test_acc_k);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBSWoyasKOOX",
        "outputId": "d7707d9d-1654-4644-f128-7e76bd713d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn acc: 0.6421185372005044\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=best_k, )\n",
        "\n",
        "classifier.fit(KNN_train[feature_names], KNN_train[target_name])\n",
        "\n",
        "sklearn_predictions = classifier.predict(KNN_test[feature_names])\n",
        "\n",
        "sklearn_acc_k = metrics.accuracy_score(KNN_test[target_name], sklearn_predictions)\n",
        "print(\"Sklearn acc:\", sklearn_acc_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWuylZxYZ6o1"
      },
      "source": [
        "Anche in questo caso le due accuracy sono praticamente uguali, con sklearn peggiore di un 0.6% circa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNeqQsbLXGo"
      },
      "source": [
        "# KNN Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROW67KNCLaiz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "KNN_samples = df.copy()\n",
        "\n",
        "KNN_samples[feature_names] = (KNN_samples[feature_names] - KNN_samples[feature_names].mean(axis=0)) / KNN_samples[feature_names].std(axis=0)\n",
        "\n",
        "KNN_train, KNN_test = train_test_split(KNN_samples, test_size=0.2 , random_state=25)\n",
        "\n",
        "KNN_validation, KNN_test = train_test_split(KNN_test, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0unHNG_-LlZl"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def KNN_reg_predict(X, distance, k):\n",
        "  predictions = []\n",
        "\n",
        "  distanze = cdist(X.values, KNN_train[feature_names].values, distance)\n",
        "  target_vicini = KNN_train[target_name].values[ np.argsort(distanze, axis = 1)[:,:k]]\n",
        "  predictions = np.mean(target_vicini, axis=1)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ennGEBFYPLS2",
        "outputId": "2a7cf848-e68c-4ee0-9413-cf58979e747b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  9 cityblock \n",
            "\tcurrent_acc = 8371.661237301116\n",
            "\tbest_acc = 8371.661237301116\n",
            "iperparametri =  27 cityblock \n",
            "\tcurrent_acc = 7900.696592688318\n",
            "\tbest_acc = 7900.696592688318\n",
            "iperparametri =  45 cityblock \n",
            "\tcurrent_acc = 7806.923342447567\n",
            "\tbest_acc = 7806.923342447567\n",
            "iperparametri =  63 cityblock \n",
            "\tcurrent_acc = 7781.064649706294\n",
            "\tbest_acc = 7781.064649706294\n",
            "iperparametri =  81 cityblock \n",
            "\tcurrent_acc = 7751.7434004829465\n",
            "\tbest_acc = 7751.7434004829465\n",
            "iperparametri =  99 cityblock \n",
            "\tcurrent_acc = 7739.9631606619805\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  9 euclidean \n",
            "\tcurrent_acc = 8447.645509625665\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  27 euclidean \n",
            "\tcurrent_acc = 7977.796641720177\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  45 euclidean \n",
            "\tcurrent_acc = 7880.135883866944\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  63 euclidean \n",
            "\tcurrent_acc = 7850.247589756319\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  81 euclidean \n",
            "\tcurrent_acc = 7823.859426851755\n",
            "\tbest_acc = 7739.9631606619805\n",
            "iperparametri =  99 euclidean \n",
            "\tcurrent_acc = 7785.120034510617\n",
            "\tbest_acc = 7739.9631606619805\n",
            "======================================================================================================\n",
            "best_iperparametri =  99 cityblock\n",
            "best = 7739.9631606619805\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "ks = [i for i in range(0,100,9)]\n",
        "distances = ['cityblock', 'euclidean']\n",
        "best_rmse = float('inf')\n",
        "\n",
        "for d in distances:\n",
        "  for k in ks:\n",
        "    if( k%2 == 0 ): continue\n",
        "    predictions = KNN_reg_predict(KNN_validation[feature_names], d, k)\n",
        "\n",
        "    current_rmse = mean_squared_error(KNN_validation['shares'], predictions, squared=False)\n",
        "\n",
        "    if( current_rmse < best_rmse):\n",
        "      best_rmse = current_rmse\n",
        "      best_k_knn, best_d_knn = k, d\n",
        "    print('iperparametri = ', k, d, '\\n\\tcurrent_acc = '+str(current_rmse)+ '\\n\\tbest_acc = '+str(best_rmse))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_k_knn, best_d_knn)\n",
        "print('best =', best_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3UqitjseAH"
      },
      "source": [
        "**Considerazioni sui risultati della Validation**\n",
        "\n",
        "Anche nel caso della regressione il K che funziona meglio è uno molto elevato. In questo caso, il motivo potrebbe essere che la colonna target ha una variabilità spiccata, il che comporta la presenza di outliers che in un \"vicinato\" piccolo, influiscono molto sulla media. Aumentando K, si riducono le distorsioni sulla media imposte da campioni che hanno un valore del target che esula dalla norma in modo poco correlato alle altre features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRqJdPPK9FVu",
        "outputId": "6c9dd6cf-ecf0-436b-d408-e0d0546c2874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6919.53871367488\n"
          ]
        }
      ],
      "source": [
        "predictions = KNN_reg_predict(KNN_test[feature_names], best_d_knn, best_k_knn)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "test_rmse_k = mean_squared_error(KNN_test[target_name], predictions, squared=False)\n",
        "print(test_rmse_k);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3yDWaZLpWNe",
        "outputId": "35df7e1d-ff48-4e54-8379-678b86fa6a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn rmse: 6944.670727529402\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=99)\n",
        "\n",
        "knn_regressor.fit(KNN_train[feature_names], KNN_train[target_name])\n",
        "\n",
        "sklearn_predictions = knn_regressor.predict(KNN_test[feature_names])\n",
        "\n",
        "sklearn_rmse_k = metrics.mean_squared_error(KNN_test[target_name], sklearn_predictions, squared = False)\n",
        "print(\"Sklearn rmse:\", sklearn_rmse_k)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJYzqriSXEoP"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "Stesso preprocessing dei dati applicato ai casi di classificazione con regressione lineare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYMaDQE2XJ2x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "Y_binary = Y_binary.flatten()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLSnuub8YLUI"
      },
      "source": [
        "expit è la funzione logistica. Essa in questo algoritmo  viene applicata dopo aver trovato una relazione lineare che interpoli i dati di training e che stimi il campione in questione.\n",
        "\n",
        "Il codominio della funzione logistica va da 0 ad 1 percui è una buona funzione per la classificazione, assegnando 1 se il risultato è maggiore di 0.5, 0 altrimenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2bNg2qYxQqs"
      },
      "outputs": [],
      "source": [
        "import scipy.special as sp\n",
        "def logistic_train(X, Y, epochs, learning_rate):\n",
        "  nCampioni, nFeatures = X.shape\n",
        "  W = np.zeros(nFeatures)\n",
        "  bias = 0\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    predicted_Y = sp.expit((np.dot(X, W)+bias))\n",
        "    W -= learning_rate * (1 / nCampioni) * np.dot(X.T, (predicted_Y - Y))\n",
        "    bias = bias - learning_rate * (1/nCampioni) * np.sum(predicted_Y - Y)\n",
        "\n",
        "  return W, bias\n",
        "\n",
        "def logistic_predict(X, W, bias):\n",
        "  predictions = 1/(1+np.exp(-(np.dot(X, W)+bias)))\n",
        "  predictions = np.where(predictions > 0.5, 1, 0)\n",
        "  return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxTxuvB8RVt1",
        "outputId": "3d1bf372-e3bf-4172-9c45-626110f37f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iperparametri =  10 0.1 \n",
            "\tcurrent_acc = 0.6321897073662966\n",
            "\tbest_acc = 0.6321897073662966\n",
            "iperparametri =  10 0.01 \n",
            "\tcurrent_acc = 0.6248738647830474\n",
            "\tbest_acc = 0.6321897073662966\n",
            "iperparametri =  10 0.001 \n",
            "\tcurrent_acc = 0.6228557013118062\n",
            "\tbest_acc = 0.6321897073662966\n",
            "iperparametri =  10 0.0001 \n",
            "\tcurrent_acc = 0.6226034308779012\n",
            "\tbest_acc = 0.6321897073662966\n",
            "iperparametri =  100 0.1 \n",
            "\tcurrent_acc = 0.643037336024218\n",
            "\tbest_acc = 0.643037336024218\n",
            "iperparametri =  100 0.01 \n",
            "\tcurrent_acc = 0.6306760847628657\n",
            "\tbest_acc = 0.643037336024218\n",
            "iperparametri =  100 0.001 \n",
            "\tcurrent_acc = 0.6248738647830474\n",
            "\tbest_acc = 0.643037336024218\n",
            "iperparametri =  100 0.0001 \n",
            "\tcurrent_acc = 0.6228557013118062\n",
            "\tbest_acc = 0.643037336024218\n",
            "iperparametri =  500 0.1 \n",
            "\tcurrent_acc = 0.6463168516649849\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  500 0.01 \n",
            "\tcurrent_acc = 0.6417759838546923\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  500 0.001 \n",
            "\tcurrent_acc = 0.627648839556004\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  500 0.0001 \n",
            "\tcurrent_acc = 0.6238647830474269\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  1000 0.1 \n",
            "\tcurrent_acc = 0.6455600403632694\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  1000 0.01 \n",
            "\tcurrent_acc = 0.6427850655903128\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  1000 0.001 \n",
            "\tcurrent_acc = 0.6309283551967709\n",
            "\tbest_acc = 0.6463168516649849\n",
            "iperparametri =  1000 0.0001 \n",
            "\tcurrent_acc = 0.6248738647830474\n",
            "\tbest_acc = 0.6463168516649849\n",
            "======================================================================================================\n",
            "best_iperparametri =  500 0.1\n",
            "best = 0.6463168516649849\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "epochses = [10, 100, 500, 1000]\n",
        "rates = [10**i for i in range(-1, -5, -1)]\n",
        "best_acc = 0\n",
        "\n",
        "\n",
        "for e in epochses:\n",
        "  for lr in rates:\n",
        "    W, bias = logistic_train(train_X, train_Y, e, lr)\n",
        "    predictions = logistic_predict(validation_X, W, bias)\n",
        "\n",
        "    current_acc = accuracy_score(validation_Y, predictions)\n",
        "\n",
        "    if( current_acc >= best_acc):\n",
        "      best_acc = current_acc\n",
        "      best_e, best_lr = e, lr\n",
        "    print('iperparametri = ', e, lr,'\\n\\tcurrent_acc = '+str(current_acc)+ '\\n\\tbest_acc = '+str(best_acc))\n",
        "\n",
        "print('======================================================================================================')\n",
        "print('best_iperparametri = ',best_e, best_lr)\n",
        "print('best =', best_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBLCKzwsuxh"
      },
      "source": [
        "**Considerazioni sui risultati della validation**\n",
        "\n",
        "Valgono le stesse fatte in precedenza, in quanto l'andamento della accuracy in funzione del numero di iterazioni massime e del learning rate è uguale al caso senza regolarizzazione. Le accuracy sono però più alte in quanto la funzione logistica funziona meglio come threshold e l'update dei pesi utilizzato in questo caso è più adatto a problemi di classificazione binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftGetOy-0uh8",
        "outputId": "3e7586a5-f81d-4ad9-f5b1-44b7ad2e1881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test acc: 0.6572509457755359\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "test_W, test_bias = logistic_train(train_X, train_Y.flatten(), best_e, best_lr)\n",
        "predictions = logistic_predict(test_X, test_W, test_bias)\n",
        "test_acc_lg = metrics.accuracy_score(test_Y, predictions)\n",
        "print(\"Test acc:\", test_acc_lg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6UHQTNIq45H",
        "outputId": "9f04f761-747f-4e85-ef3d-690806a37fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SKlearn acc: 0.6622950819672131\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_regression = LogisticRegression()\n",
        "\n",
        "logistic_regression.fit(train_X, train_Y)\n",
        "\n",
        "sklearn_predictions = logistic_regression.predict(test_X)\n",
        "\n",
        "sklearn_acc_lg = metrics.accuracy_score(test_Y, sklearn_predictions)\n",
        "\n",
        "print(\"SKlearn acc:\", sklearn_acc_lg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSBXgtrOACFM"
      },
      "source": [
        "# Neural Networks Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3uf1Lg3fo2x"
      },
      "outputs": [],
      "source": [
        "def inizializzazione(nNeuroni):\n",
        "    W1 = np.random.rand(58, nNeuroni)\n",
        "    B1 = np.zeros((1, nNeuroni))\n",
        "    W2 = np.random.rand(nNeuroni, 1)\n",
        "    B2 = np.zeros((1, 1))\n",
        "    return W1, B1, W2, B2\n",
        "\n",
        "from scipy.special import expit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HErMAbC4hLOJ"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(W1, B1, W2, B2, X):\n",
        "\n",
        "  hidden_Z = np.dot(X, W1) + B1\n",
        "  hidden_Y = expit(hidden_Z)\n",
        "  output_Z = np.dot(hidden_Y, W2) + B2\n",
        "  output_Y = output_Z\n",
        "\n",
        "  return hidden_Z, hidden_Y, output_Z, output_Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RplgCf6whjpC"
      },
      "outputs": [],
      "source": [
        "def backpropagation(W1, B1, W2, B2, X, Y, hidden_Y, output_Y):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Backpropagation sull' output layer\n",
        "    dZ2 = output_Y - Y\n",
        "    dW2 = (1 / m) * np.dot(hidden_Y.T, dZ2)\n",
        "    dB2 = (1 / m) * np.sum(dZ2, axis=0)\n",
        "\n",
        "    # Backpropagation sull' hidden layer\n",
        "    dZ1 = np.dot(dZ2, W2.T) * hidden_Y * (1 - hidden_Y)\n",
        "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
        "    dB1 = (1 / m) * np.sum(dZ1, axis=0)\n",
        "\n",
        "    return dW1, dB1, dW2, dB2\n",
        "\n",
        "def update(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "  W1 -= learning_rate * dW1\n",
        "  b1 -= learning_rate * db1\n",
        "  W2 -= learning_rate * dW2\n",
        "  b2 -= learning_rate * db2\n",
        "  return W1, b1, W2, b2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i9em8ttGqOV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdAgCntsmdc_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def gradient_descent(X, Y, learning_rate, epochs, neurons):\n",
        "  W1, B1, W2, B2 = inizializzazione(neurons)\n",
        "  for i in range(epochs):\n",
        "    Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)\n",
        "    dW1, db1, dW2, db2 = backpropagation(W1, B1, W2, B2, X, Y, A1, A2)\n",
        "    W1, B1, W2, B2 = update(W1, B1, W2, B2, dW1, db1, dW2, db2, learning_rate)\n",
        "  return W1, B1, W2, B2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0vGfwImdeub",
        "outputId": "2eafa938-94d5-497f-9e70-079a6b95a8b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================TRYING neurons: 10 epochs: 1000 learning rate: 0.1\n",
            "validaton RMSE: 7732.2849687516555\n",
            "best RMSE: 7732.2849687516555 with neurons: 10 epochs: 1000 learning rate: 0.1\n",
            "==================TRYING neurons: 10 epochs: 1000 learning rate: 0.001\n",
            "validaton RMSE: 7724.848578719618\n",
            "best RMSE: 7724.848578719618 with neurons: 10 epochs: 1000 learning rate: 0.001\n",
            "==================TRYING neurons: 10 epochs: 2500 learning rate: 0.1\n",
            "validaton RMSE: 7725.12200598591\n",
            "best RMSE: 7724.848578719618 with neurons: 10 epochs: 1000 learning rate: 0.001\n",
            "==================TRYING neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "validaton RMSE: 7695.384541867297\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 20 epochs: 1000 learning rate: 0.1\n",
            "validaton RMSE: 7716.244876033896\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 20 epochs: 1000 learning rate: 0.001\n",
            "validaton RMSE: 7703.176844404595\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 20 epochs: 2500 learning rate: 0.1\n",
            "validaton RMSE: 7714.102297182806\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 20 epochs: 2500 learning rate: 0.001\n",
            "validaton RMSE: 7702.973025823331\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 50 epochs: 1000 learning rate: 0.1\n",
            "validaton RMSE: 7727.998598679277\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 50 epochs: 1000 learning rate: 0.001\n",
            "validaton RMSE: 7697.087955959178\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 50 epochs: 2500 learning rate: 0.1\n",
            "validaton RMSE: 7752.520840909148\n",
            "best RMSE: 7695.384541867297 with neurons: 10 epochs: 2500 learning rate: 0.001\n",
            "==================TRYING neurons: 50 epochs: 2500 learning rate: 0.001\n",
            "validaton RMSE: 7687.2035470890305\n",
            "best RMSE: 7687.2035470890305 with neurons: 50 epochs: 2500 learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "\n",
        "neurons = [ 10, 20, 50]\n",
        "epochses = [1000, 2500]\n",
        "learning = [0.1, 0.001]\n",
        "\n",
        "best = float('inf')\n",
        "top = [0,0,0]\n",
        "for n in neurons:\n",
        "  for e in epochses:\n",
        "    for lr in learning:\n",
        "      print(\"==================TRYING neurons:\", n, \"epochs:\", e, \"learning rate:\", lr)\n",
        "      W1, b1, W2, b2 = gradient_descent(train_X.values, train_Y.values, lr, e, n)\n",
        "      Z1, A1, Z2, A2 = forward_propagation( W1, b1, W2, b2, validation_X.values )\n",
        "      rmse = mean_squared_error(validation_Y, A2, squared=False)\n",
        "      print(\"validaton RMSE:\", rmse)\n",
        "      if(rmse<best):\n",
        "          best = rmse\n",
        "          top = [ n, e, lr]\n",
        "      print(\"best RMSE:\", best, 'with neurons:', top[0], 'epochs:', top[1], 'learning rate:', top[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjMwHdJxtdyx"
      },
      "source": [
        "**Considerazioni sui risultati del validation**\n",
        "\n",
        "In questo caso notiamo un andamento a double dip. Il modello in questione in fatti non è lineare: i neuroni aggiungono una non linearità tramite la funzione di attivazione (sigmoide in questo caso).\n",
        "Il double dip è dovuto al fatto che un aumento di complessità inizialmente porta la rete ad overfittare. Successivamente però l'aumento di complessità apre alla possibilità di funzioni più accurate che rappresentino il problema meglio di quelle trovate prima dell'aumento di complessità"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arn7ZpS-oHV1",
        "outputId": "4c832608-3045-4c63-90d1-b268f222bcd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test RMSE: 6951.529363892703\n"
          ]
        }
      ],
      "source": [
        "W1, b1, W2, b2 = gradient_descent(train_X.values, train_Y.values, 0.1, 2500, 30)\n",
        "Z1, A1, Z2, A2 = forward_propagation( W1, b1, W2, b2, test_X.values )\n",
        "test_rmse_nn = mean_squared_error(test_Y, A2, squared=False)\n",
        "print(\"test RMSE:\", test_rmse_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfAJBT11992b",
        "outputId": "660009d8-1eae-47f3-8ca6-efeacd80a13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn rmse: 7198.223016916968\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "reg = MLPRegressor(hidden_layer_sizes=(10), activation='logistic', solver='sgd', random_state=42, learning_rate = 'constant', max_iter=2500, learning_rate_init=0.1)\n",
        "\n",
        "reg.fit(train_X, train_Y.values.flatten())\n",
        "\n",
        "y_pred = reg.predict(test_X)\n",
        "\n",
        "sklearn_rmse_nn = mean_squared_error(test_Y.values, y_pred, squared=False)\n",
        "print(\"Sklearn rmse:\", sklearn_rmse_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4J61IhGazMP"
      },
      "source": [
        "# Neural Networks Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgeSMr1ga3aM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_names = [df.columns[i] for i in range(0,58)]\n",
        "target_name = df.columns[58]\n",
        "\n",
        "X=df[feature_names].copy()\n",
        "Y=df[[target_name]].copy()\n",
        "median = Y.median()\n",
        "Y_binary = np.where(Y >= median, 1, 0)\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y_binary, test_size=0.2 , random_state=25)\n",
        "\n",
        "validation_X, test_X, validation_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMBYRlCddEDL"
      },
      "outputs": [],
      "source": [
        "def inizializzazione(nNeuroni):\n",
        "    W1 = np.random.rand(58, nNeuroni)\n",
        "    B1 = np.zeros((1, nNeuroni))\n",
        "    W2 = np.random.rand(nNeuroni, 1)\n",
        "    B2 = np.zeros((1, 1))\n",
        "    return W1, B1, W2, B2\n",
        "\n",
        "from scipy.special import expit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmTeGuIVdEDM"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(W1, B1, W2, B2, X):\n",
        "\n",
        "  hidden_Z = np.dot(X, W1) + B1\n",
        "  hidden_Y = expit(hidden_Z)\n",
        "  output_Z = np.dot(hidden_Y, W2) + B2\n",
        "  output_Y = expit(output_Z)\n",
        "\n",
        "\n",
        "  return hidden_Z, hidden_Y, output_Z, output_Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjW7_lRfdEDN"
      },
      "outputs": [],
      "source": [
        "def backpropagation(W1, B1, W2, B2, X, Y, hidden_Y, output_Y):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Backpropagation sull' output layer\n",
        "    dA2 = output_Y - Y\n",
        "    dZ2 = dA2 * output_Y * (1 - output_Y)\n",
        "    dW2 = (1 / m) * np.dot(hidden_Y.T, dZ2)\n",
        "    dB2 = (1 / m) * np.sum(dZ2, axis=0)\n",
        "\n",
        "    # Backpropagation sull'hidden layer\n",
        "    dZ1 = np.dot(dZ2, W2.T) * hidden_Y * (1 - hidden_Y)\n",
        "    dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
        "    dB1 = (1 / m) * np.sum(dZ1, axis=0)\n",
        "\n",
        "    return dW1, dB1, dW2, dB2\n",
        "\n",
        "def update(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "  W1 -= learning_rate * dW1\n",
        "  b1 -= learning_rate * db1\n",
        "  W2 -= learning_rate * dW2\n",
        "  b2 -= learning_rate * db2\n",
        "  return W1, b1, W2, b2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq_9O4_td7d6"
      },
      "outputs": [],
      "source": [
        "def nn_clas_predict(output_Y):\n",
        "  return np.where(output_Y > 0.5, 1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCOVHUGTd0Dc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def gradient_descent(X, Y, learning_rate, epochs, neurons):\n",
        "  W1, B1, W2, B2 = inizializzazione(neurons)\n",
        "  for i in range(epochs):\n",
        "    Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)\n",
        "    dW1, db1, dW2, db2 = backpropagation(W1, B1, W2, B2, X, Y, A1, A2)\n",
        "    W1, B1, W2, B2 = update(W1, B1, W2, B2, dW1, db1, dW2, db2, learning_rate)\n",
        "  return W1, B1, W2, B2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7UMaRnld6MB",
        "outputId": "33c56cf1-1e59-46ef-ccbd-fd914f2d4764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================TRYING neurons: 5 epochs: 1000 learning rate: 0.1\n",
            "validaton acc: 0.6062058526740666\n",
            "best acc: 0.6062058526740666 with neurons: 5 epochs: 1000 learning rate: 0.1\n",
            "==================TRYING neurons: 5 epochs: 1000 learning rate: 0.01\n",
            "validaton acc: 0.5726538849646822\n",
            "best acc: 0.6062058526740666 with neurons: 5 epochs: 1000 learning rate: 0.1\n",
            "==================TRYING neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "validaton acc: 0.6157921291624622\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 5 epochs: 2500 learning rate: 0.01\n",
            "validaton acc: 0.5973763874873865\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 10 epochs: 1000 learning rate: 0.1\n",
            "validaton acc: 0.5940968718466195\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 10 epochs: 1000 learning rate: 0.01\n",
            "validaton acc: 0.5476791120080726\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 10 epochs: 2500 learning rate: 0.1\n",
            "validaton acc: 0.6112512613521696\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 10 epochs: 2500 learning rate: 0.01\n",
            "validaton acc: 0.5946014127144299\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 30 epochs: 1000 learning rate: 0.1\n",
            "validaton acc: 0.5688698284561049\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 30 epochs: 1000 learning rate: 0.01\n",
            "validaton acc: 0.5325428859737639\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 30 epochs: 2500 learning rate: 0.1\n",
            "validaton acc: 0.6099899091826438\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n",
            "==================TRYING neurons: 30 epochs: 2500 learning rate: 0.01\n",
            "validaton acc: 0.5428859737638748\n",
            "best acc: 0.6157921291624622 with neurons: 5 epochs: 2500 learning rate: 0.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "neurons = [5, 10, 30]\n",
        "epochses = [1000, 2500]\n",
        "learning = [0.1, 0.01]\n",
        "\n",
        "best = 0\n",
        "best_e, best_lr, best_n = 0,0,0\n",
        "for n in neurons:\n",
        "  for e in epochses:\n",
        "    for lr in learning:\n",
        "        print(\"==================TRYING neurons:\", n, \"epochs:\", e, \"learning rate:\", lr)\n",
        "        W1, b1, W2, b2 = gradient_descent(train_X.values, train_Y, lr, e, n)\n",
        "        Z1, A1, Z2, A2 = forward_propagation( W1, b1, W2, b2, validation_X.values )\n",
        "        acc = accuracy_score(validation_Y, nn_clas_predict(A2))\n",
        "        print(\"validaton acc:\", acc)\n",
        "        if(acc>best):\n",
        "            best = acc\n",
        "            best_e, best_lr, best_n = e, lr, n\n",
        "        print(\"best acc:\", best, 'with neurons:', best_n, 'epochs:', best_e, 'learning rate:', best_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKUooPTgeumS",
        "outputId": "66d84ded-ab7c-49ce-90f4-604e9c9d2c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test acc: 0.6075662042875157\n"
          ]
        }
      ],
      "source": [
        "W1, b1, W2, b2 = gradient_descent(train_X.values, train_Y, best_lr, best_e, best_n)\n",
        "Z1, A1, Z2, A2 = forward_propagation( W1, b1, W2, b2, test_X.values )\n",
        "predictions = nn_clas_predict(A2)\n",
        "test_acc_nn = accuracy_score(test_Y, predictions)\n",
        "print(\"test acc:\", test_acc_nn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ae7lDXu8B7p",
        "outputId": "32aa2f4c-dfff-44d0-ff27-7a935e4d5a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn Accuracy: 0.6307692307692307\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = MLPClassifier(hidden_layer_sizes=(30), activation='logistic', solver='sgd', random_state=42, learning_rate = 'constant', max_iter=5000, learning_rate_init=0.1)\n",
        "\n",
        "clf.fit(train_X, train_Y.flatten())\n",
        "\n",
        "y_pred = clf.predict(test_X)\n",
        "\n",
        "sklearn_acc_nn = accuracy_score(test_Y, y_pred)\n",
        "print(\"Sklearn Accuracy:\", sklearn_acc_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JEP9r-jvPxL"
      },
      "source": [
        "# Confronti finali rispetto alle implementazioni di sci-kit learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "s2Bw3pqA0zAE",
        "outputId": "ee4ff279-5a60-40b7-b20c-19bd91b93a47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f64a65b5-2368-4dc0-9dce-211a1cc8d0d2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metodo</th>\n",
              "      <th>Miei</th>\n",
              "      <th>Sklearn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.614327</td>\n",
              "      <td>0.614264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lineare</td>\n",
              "      <td>0.615637</td>\n",
              "      <td>0.639344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lineare con L2</td>\n",
              "      <td>0.640858</td>\n",
              "      <td>0.639344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lineare con L1</td>\n",
              "      <td>0.626986</td>\n",
              "      <td>0.645397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lineare ElasticNet</td>\n",
              "      <td>0.642119</td>\n",
              "      <td>0.630517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Logistic</td>\n",
              "      <td>0.657251</td>\n",
              "      <td>0.662295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.648928</td>\n",
              "      <td>0.642119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Neural Nets</td>\n",
              "      <td>0.607566</td>\n",
              "      <td>0.630769</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f64a65b5-2368-4dc0-9dce-211a1cc8d0d2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f64a65b5-2368-4dc0-9dce-211a1cc8d0d2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f64a65b5-2368-4dc0-9dce-211a1cc8d0d2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Metodo      Miei   Sklearn\n",
              "0       Decision Tree  0.614327  0.614264\n",
              "1             Lineare  0.615637  0.639344\n",
              "2      Lineare con L2  0.640858  0.639344\n",
              "3      Lineare con L1  0.626986  0.645397\n",
              "4  Lineare ElasticNet  0.642119  0.630517\n",
              "5            Logistic  0.657251  0.662295\n",
              "6                 KNN  0.648928  0.642119\n",
              "7         Neural Nets  0.607566  0.630769"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "summary_acc = pd.DataFrame([['Decision Tree', test_acc_dt, sklearn_acc_dt], ['Lineare', test_acc, sklearn_acc], ['Lineare con L2', test_acc_r, sklearn_acc], ['Lineare con L1', test_acc_l, sklearn_acc_l], ['Lineare ElasticNet', test_acc_e, sklearn_acc_e], ['Logistic', test_acc_lg, sklearn_acc_lg], ['KNN', test_acc_k, sklearn_acc_k], ['Neural Nets', test_acc_nn, sklearn_acc_nn]], columns=['Metodo','Miei', 'Sklearn'])\n",
        "display(summary_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osyOOceBTgcN"
      },
      "source": [
        "Si nota come le implementazioni proposte, rispetto all'implementazione di sci-kit learn non sono molto peggiori in termini di accuracy. Anzi, nella metà dei casi sono addirittura migliori. In generale comunque la differenza massima resta intorno al 2%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "rJ4sSmCuzh2e",
        "outputId": "575427b3-fcea-4369-935a-49d8790c4daf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-731ae28c-5932-473a-8c6c-610df9e74e8c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metodo</th>\n",
              "      <th>Miei</th>\n",
              "      <th>Sklearn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lineare</td>\n",
              "      <td>6877.960398</td>\n",
              "      <td>6875.090035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lineare con L2</td>\n",
              "      <td>6873.572182</td>\n",
              "      <td>6875.090035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lineare con L1</td>\n",
              "      <td>6877.223863</td>\n",
              "      <td>6877.953924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lineare ElasticNet</td>\n",
              "      <td>6877.224969</td>\n",
              "      <td>6885.141697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>6919.538714</td>\n",
              "      <td>6944.670728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Neural Nets</td>\n",
              "      <td>6951.529364</td>\n",
              "      <td>7198.223017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-731ae28c-5932-473a-8c6c-610df9e74e8c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-731ae28c-5932-473a-8c6c-610df9e74e8c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-731ae28c-5932-473a-8c6c-610df9e74e8c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Metodo         Miei      Sklearn\n",
              "0             Lineare  6877.960398  6875.090035\n",
              "1      Lineare con L2  6873.572182  6875.090035\n",
              "2      Lineare con L1  6877.223863  6877.953924\n",
              "3  Lineare ElasticNet  6877.224969  6885.141697\n",
              "4                 KNN  6919.538714  6944.670728\n",
              "5         Neural Nets  6951.529364  7198.223017"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "summary_rmse = pd.DataFrame([['Lineare', test_rmse, sklearn_rmse], ['Lineare con L2', test_rmse_r, sklearn_rmse], ['Lineare con L1', test_rmse_l, sklearn_rmse_l], ['Lineare ElasticNet', test_rmse_e, sklearn_rmse_e], ['KNN', test_rmse_k, sklearn_rmse_k], ['Neural Nets', test_rmse_nn, sklearn_rmse_nn]], columns=['Metodo','Miei', 'Sklearn'])\n",
        "display(summary_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiyD0bAEUEED"
      },
      "source": [
        "Anche nel caso della regressione l'rmse riportato dall'implementazione proposta non è peggiore di quello di sklearn. Anzi è quasi sempre minore, unica eccezione nel caso di regressione lineare senza regolarizzazione in cui l'rmse di sklearn è minore di 2.9, a fronte di un ordine di grandezza delle migliaia della colonna target da stimare.\n",
        "\n",
        "Questi risultati indicano una corretta implementazione. Si può verificare anche che modificando gli iperparametri, le performance tendono a peggiorare sia per l'implementazione proposta, sia per quella di sci kit learn, situazione indicatrice di una scelta accurata degli iperparametri.\n",
        "\n",
        "\n",
        "\n",
        "#Confronti tra i modelli per la regressione lineare**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "JnK0ZqUwU9gx",
        "outputId": "79471b2f-5ead-4e3a-fe20-696f677be0c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-068dfa8b-22d4-47b3-9595-99e0bc973220\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metodo</th>\n",
              "      <th>Miei</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lineare</td>\n",
              "      <td>6877.960398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lineare con L2</td>\n",
              "      <td>6873.572182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lineare con L1</td>\n",
              "      <td>6877.223863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lineare ElasticNet</td>\n",
              "      <td>6877.224969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>6919.538714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Neural Nets</td>\n",
              "      <td>6951.529364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-068dfa8b-22d4-47b3-9595-99e0bc973220')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-068dfa8b-22d4-47b3-9595-99e0bc973220 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-068dfa8b-22d4-47b3-9595-99e0bc973220');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Metodo         Miei\n",
              "0             Lineare  6877.960398\n",
              "1      Lineare con L2  6873.572182\n",
              "2      Lineare con L1  6877.223863\n",
              "3  Lineare ElasticNet  6877.224969\n",
              "4                 KNN  6919.538714\n",
              "5         Neural Nets  6951.529364"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(summary_rmse[['Metodo', 'Miei']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-6LB_dnVRLH"
      },
      "source": [
        "Notiamo che i modelli non lineari (KNN e Reti neurali) hanno ottenuto performance peggiori rispetto ai modelli non lineari. Questi risultati potrebbero indicare che la relazione tra le feature predittive e quella target potrebbe essere effettivamente vicina a una relazione lineare e viene meglio rappresentata da modelli lineari.\n",
        "\n",
        "Le peggiori performance delle reti neurali potrebbero essere dovute anche al fatto che esse hanno bisogno di una quantità di dati molto alta per apprendere relazioni più complesse e il dataset composto da meno di 40 mila campioni potrebbe non essere stato sufficiente. Questo potrebbe spiegare anche perchè tra i modelli non lineari le neural networks performino peggio del KNN.\n",
        "\n",
        "Confontiamo tra loro i modelli lineari e notiamo che:\n",
        "Tra le regolarizzazioni, quella che ci da il risultato migliore, anche se di poco, è la regolarizzazione Ridge. Uno scenario del genere può suggerire che nel dataset c'è Multicollinearità ovvero quando più predittori sono strettamente correlati tra loro. In questo caso potrebbe essere ciò che succede a features di cui abbiamo una misura massima, minima, e media nel dataset (tutte grandezze tra loro correlate).\n",
        "Inoltre la Elastic net presenta un errore più ampio sia della regolarizzazione L1, sia di quella L2. Questo non sorprende perchè, se L1 da sola performava peggio di L2 da sola, aggiungere un termine L1 a quello L2 aggiunge anche una distorsione senza introdurre particolari benefici, ancora una volta a confermare il fatto che nel dataset ci sono caratteristiche più affini ad una regolarizzazione quadratica che mantenga il peso di ogni feature senza eliminarne alcuna.\n",
        "\n",
        "#Confronti tra i vari modelli di Classificazione\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "yrrnaR3wYw3f",
        "outputId": "b5c89572-5406-4073-dcfc-c06ebbb1de6b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2e9af44b-859f-470e-a931-c28589d53ffc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metodo</th>\n",
              "      <th>Miei</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>0.614327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lineare</td>\n",
              "      <td>0.615637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lineare con L2</td>\n",
              "      <td>0.640858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lineare con L1</td>\n",
              "      <td>0.626986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lineare ElasticNet</td>\n",
              "      <td>0.642119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Logistic</td>\n",
              "      <td>0.657251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.648928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Neural Nets</td>\n",
              "      <td>0.607566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e9af44b-859f-470e-a931-c28589d53ffc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e9af44b-859f-470e-a931-c28589d53ffc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e9af44b-859f-470e-a931-c28589d53ffc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Metodo      Miei\n",
              "0       Decision Tree  0.614327\n",
              "1             Lineare  0.615637\n",
              "2      Lineare con L2  0.640858\n",
              "3      Lineare con L1  0.626986\n",
              "4  Lineare ElasticNet  0.642119\n",
              "5            Logistic  0.657251\n",
              "6                 KNN  0.648928\n",
              "7         Neural Nets  0.607566"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(summary_acc[['Metodo', 'Miei']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vrqS6COYyaY"
      },
      "source": [
        "\n",
        "Notevole come la regressione logistica funzioni meglio delle regressioni lineari per problemi di classificazione in virtù della più sofisticata regola per gli updates che tiene conto di una funzione di costo progettata appositamente per problemi di regressione binaria e che considera l'output della funzione logistica come la probabilità che la classe di quel campione sia una piuttosto che l'altra.\n",
        "\n",
        "In generale i modelli lineari sembrano funzionare meglio di quelli non lineari anche in questo caso, con le Neural Networks che hanno un distacco più pronunciato, possibilmente per gli stessi motivi per i quali si osservava questa differenza di rmse nel problema di regressione. La maggiore accuracy dei modelli lineari suggericse che i decision boundaries siano sono lineari e che le classi siano ben separate linearmente. Questo è ulteriormente suggerito dall'ottimo risultato ottenuto dall'algoritmo KNN anche rispetto ai modelli lineari usando un valore di K molto grande (99), per i motivi già discussi nella sezione validation del KNN.\n",
        "\n",
        "In generale gli rmse ottenuti sono piuttosto alti e le accuracy mai oltre il 65%. Questo suggerisce che il dataset sia più complesso di quanto i modelli usati riescano a rappresentare e tende a verificarsi underfit. Questo effetto è particolarmente pronunciato nel caso dei Decision Tree che sono modelli molto semplici non particolarmente adatti a carpire le complessità di un insieme di dati."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
